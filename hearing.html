<!DOCTYPE html><html lang="ko"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><title>청각(Hearing) - noba-ko</title><meta name="description" content="청각 By Andrew J. Oxenham University of Minnesota 청각은 우리 주변의 음향 진동의 세계를 인식할 수 있게 해주며, 가장 중요한 의사소통 채널을 제공합니다."><meta name="generator" content="Publii Open-Source CMS for Static Site"><script type="text/javascript" async src="https://www.googletagmanager.com/gtag/js?id=G-QFEP3D8PNE"></script><script type="text/javascript">window.dataLayer = window.dataLayer || [];
				  function gtag(){dataLayer.push(arguments);}
				  gtag('js', new Date());
				  gtag('config', 'G-QFEP3D8PNE' );</script><link rel="canonical" href="https://learningjunho.github.io/noba-ko/hearing.html"><link rel="alternate" type="application/atom+xml" href="https://learningjunho.github.io/noba-ko/feed.xml"><link rel="alternate" type="application/json" href="https://learningjunho.github.io/noba-ko/feed.json"><meta property="og:title" content="청각(Hearing)"><meta property="og:site_name" content="noba-ko"><meta property="og:description" content="청각 By Andrew J. Oxenham University of Minnesota 청각은 우리 주변의 음향 진동의 세계를 인식할 수 있게 해주며, 가장 중요한 의사소통 채널을 제공합니다."><meta property="og:url" content="https://learningjunho.github.io/noba-ko/hearing.html"><meta property="og:type" content="article"><link rel="stylesheet" href="https://learningjunho.github.io/noba-ko/assets/css/fontawesome-all.min.css?v=dbf9d822cefe851ba6f66e1ad57e8987"><link rel="stylesheet" href="https://learningjunho.github.io/noba-ko/assets/css/style.css?v=ab9404217d48a7e03be67391f59606ae"><noscript><link rel="stylesheet" href="https://learningjunho.github.io/noba-ko/assets/css/noscript.css?v=6228c7eee614cd200a2cad8333b439fa"></noscript><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://learningjunho.github.io/noba-ko/hearing.html"},"headline":"청각(Hearing)","datePublished":"2023-07-02T06:38","dateModified":"2023-07-03T20:05","description":"청각 By Andrew J. Oxenham University of Minnesota 청각은 우리 주변의 음향 진동의 세계를 인식할 수 있게 해주며, 가장 중요한 의사소통 채널을 제공합니다.","author":{"@type":"Person","name":"learningjunho","url":"https://learningjunho.github.io/noba-ko/authors/learningjunho/"},"publisher":{"@type":"Organization","name":"learningjunho"}}</script><style>#wrapper > .bg {
               background-image: url(https://learningjunho.github.io/noba-ko/assets/images/overlay.png), linear-gradient(0deg, rgba(0, 0, 0, 0.1), rgba(0, 0, 0, 0.1)), url();
           }</style><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript></head><body class="is-preload"><div id="wrapper"><header id="header"><a class="logo" href="https://learningjunho.github.io/noba-ko/">noba-ko</a></header><nav id="nav"><ul class="links"><li><a href="https://learningjunho.github.io/noba-ko/" target="_self">홈</a></li><li><a href="https://learningjunho.github.io/noba-ko/tags/beonyeog-wanryo/" target="_self">번역 완료</a></li></ul></nav><main id="main"><article class="post"><header class="major"><time datetime="2023-07-02T06:38" class="date">7월 2, 2023</time><h1>청각(Hearing)</h1><p class="post__inner"></p></header><div class="post__inner post__entry"><article class="noba-chapter noba-chapter-enhanced"><header id="abstract"><h1>청각</h1>By <a href="https://nobaproject.com/authors/andrew-j-oxenham" rel="author">Andrew J. Oxenham</a><p class="text-muted">University of Minnesota</p></header><section><p class="lead">청각은 우리 주변의 음향 진동의 세계를 인식할 수 있게 해주며, 가장 중요한 의사소통 채널을 제공합니다. 이 모듈에서는 귀의 해부학 및 생리학부터 시작하여 청각의 기본 메커니즘을 살펴보고 청각 피질에 이르는 청각 경로에 대해 간략하게 살펴봅니다. 음량, 음높이, 음색 등 소리의 기본 지각 속성에 대한 개요를 살펴본 다음 달팽이관에 확립된 토노토픽 조직의 원리를 검토합니다. 마스킹과 주파수 선택성에 대한 개요에 이어 공간 청각의 기초가 되는 지각과 신경 메커니즘에 대한 검토가 이어집니다. 마지막으로 청각 장면 분석에 대한 개요를 통해 청각 시스템이 일상적인 음향 환경에서 접하는 복잡한 소리 혼합을 어떻게 이해할 수 있는지에 대한 중요한 질문을 다룹니다.</p></section><nav class="navbar navbar-default noba-navbar-action" role="toolbar" aria-label="Share &amp; Download Module"><div class="navbar-header"> </div><div id="noba-navbar-action" class="collapse navbar-collapse"><div class="collapse-wrapper"><ul class="nav navbar-nav"><li><a class="share share-facebook" data-share-receipt-service="facebook" data-share-receipt-url="https://nobaproject.com/modules/hearing/share" href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Fnoba.to%2Fjry3cu78" onclick="window.open(this.href, '', 'width=626,height=436');"><img loading="lazy" title="Share on Facebook" src="https://nobaproject.com/assets/social/share/facebook@2x-14b7f010fdaab7751eaff49c702d45851296a56967f4fbdc12170671bf594d7f.png" alt="Share on Facebook" width="42" height="42" data-is-external-image="true"></a></li><li><a class="share share-twitter" data-share-receipt-service="twitter" data-share-receipt-url="https://nobaproject.com/modules/hearing/share" href="https://twitter.com/intent/tweet?url=http%3A%2F%2Fnoba.to%2Fjry3cu78&amp;text=This%20is%20the%20future%20of%20textbooks%3A%C2%A0Hearing" onclick="window.open(this.href, '', 'width=550,height=460,scrollbars=true');"><img loading="lazy" title="Share on Twitter" src="https://nobaproject.com/assets/social/share/twitter@2x-f5c62500fccc964637cab4e89196b7c44e7ef64b8654ee123dfd48448232bd52.png" alt="Share on Twitter" width="42" height="42" data-is-external-image="true"></a></li><li><a class="share share-email" data-toggle="modal" href="https://nobaproject.com/modules/hearing#modal-email_16"><img loading="lazy" title="Share via Email" src="https://nobaproject.com/assets/social/share/email@2x-a8982d45249ff3b3e437d1f1125dd1faf1e5649f091e9c7eabfb1a8a1a3c4521.png" alt="Share via Email" width="42" height="42" data-is-external-image="true"></a></li></ul><form class="navbar-form navbar-left share share-url"><label class="sr-only">Share this URL</label><input class="form-control" readonly="readonly" type="text" value="http://noba.to/jry3cu78"></form></div></div></nav><section id="tags"><ul class="tags"><li><a href="https://nobaproject.com/browse-content?tags=225">Auditory perception</a></li><li><a href="https://nobaproject.com/browse-content?tags=227">Auditory scene analysis</a></li><li><a href="https://nobaproject.com/browse-content?tags=226">Frequency selectivity</a></li><li><a href="https://nobaproject.com/browse-content?tags=224">Hearing</a></li></ul></section><section><h2 id="learning-objectives">학습 목표</h2><ul><li>Describe the basic auditory attributes of sound.</li><li>Describe the structure and general function of the auditory pathways from the outer ear to the auditory cortex.</li><li>Discuss ways in which we are able to locate sounds in space.</li><li>Describe various acoustic cues that contribute to our ability to perceptually segregate simultaneously arriving sounds.</li></ul><ul><li>소리의 기본적인 청각적 속성을 설명합니다.<br>외이에서 청각 피질에 이르는 청각 경로의 구조와 일반적인 기능을 설명합니다.<br>공간에서 소리를 찾을 수 있는 방법에 대해 토론합니다.<br>동시에 도달하는 소리를 지각적으로 분리하는 능력에 기여하는 다양한 청각 단서에 대해 설명합니다.</li></ul></section><section class="content"><h2 id="introduction">Introduction</h2><figure data-align="right"><img loading="lazy" title="A young boy whispers in the ear of a young girl." src="https://nobaproject.com/images/shared/images/000/002/182/original.jpg" alt="A young boy whispers in the ear of a young girl." data-is-external-image="true"><figcaption>Hearing provides us with our most important connection to the people around us. [Image: Bindaas Madhavi, https://goo.gl/Sv6TtR, CC BY-NC-ND 2.0, https://goo.gl/62XJAl]</figcaption></figure><p>청각은 일상 생활에서 중요한 부분을 차지합니다. 말이나 음악을 통해 다른 사람과의 의사소통은 대부분 귀를 통해 이루어집니다. 헬렌 켈러가 자주 인용하는 속담에 따르면, 실명은 우리를 사물과 분리시키지만 청각 장애는 우리를 사람과 분리시킨다고 합니다. 귀는 음향 정보, 즉 기압의 작고 빠른 변화와 같은 소리에 반응합니다. 음파는 소스에서 이동하여 청취자의 외이도에 압력 변화를 일으켜 고막(또는 고막)을 진동시킵니다. 이 모듈에서는 이러한 단순한 기계적 진동을 청각 또는 청각적 지각이라는 풍부한 경험으로 변환하는 일련의 과정에 대한 개요를 제공합니다.</p><h1 id="perceptual-attributes-of-sound">소리의 지각적 속성</h1><p>소리를 설명하는 방법은 여러 가지가 있지만, 일반적으로 소리의 지각 속성은 크게 음량, 음높이, 음색이라는 세 가지 범주로 나눌 수 있습니다. 이 세 가지 모두 물리적 소리 자체가 아니라 지각에 관한 것이지만, 다양한 물리적 변수와 밀접한 관련이 있습니다.</p><h2 id="loudness">음량</h2><p>음량과 가장 직접적인 물리적 상관관계는 고막 가까이에서 측정한 음 강도(또는 음압)입니다. 그러나 주파수 내용, 지속 시간, 소리가 전달되는 맥락 등 다른 많은 요인도 소리의 크기에 영향을 미칩니다. 청각 지각에 대한 초기의 심리물리학적 연구 중 일부는 한 세기 이상 거슬러 올라가는데, 지각된 음량, 물리적 소리 강도 및 눈에 띄는 음량 차이 사이의 관계를 조사하는 데 목적이 있었습니다(Fechner, 1860; Stevens, 1957). 다양한 측정 방법을 개선하는 데 많은 시간과 노력을 기울여 왔습니다. 이러한 방법에는 크기 추정과 같은 기법이 포함되는데, 일련의 소리(보통 정현파 또는 단일 주파수의 순수한 음색)를 서로 다른 음량으로 순차적으로 제시하고 피험자에게 인지된 음량에 해당하는 숫자를 각 음색에 할당하도록 요청하는 방식입니다. 다른 연구에서는 톤의 주파수에 따라 음량이 어떻게 변하는지를 조사하여 국제 표준인 ISO 음량 수준 윤곽선(ISO, 2003)을 만들었으며, 이 윤곽선은 여러 산업 분야에서 소음 및 성가심 문제를 평가하는 데 사용되고 있습니다. 이러한 연구를 통해 임의의 소리의 음량을 예측하도록 설계된 계산 모델이 개발되었습니다(예: Moore, Glasberg, &amp; Baer, 1997).</p><h2 id="pitch">피치</h2><figure data-align="left"><img loading="lazy" title="A man plays a piano on a sidewalk." src="https://nobaproject.com/images/shared/images/000/002/185/original.jpg" alt="A man plays a piano on a sidewalk." data-is-external-image="true"><figcaption>Pitch is crucial to our perception and understanding of music and language. [Image: xroper7, https://goo.gl/1E4sJY, CC BY-NC 2.0, https://goo.gl/tgFydH]</figcaption></figure><p>피치는 청각적 의사소통에서 중요한 역할을 합니다. 시간에 따른 피치 변화는 대부분의 음악에서 멜로디의 기초를 제공하며, 말의 피치 윤곽은 영어와 같은 비조 언어에서 중요한 운율 정보를 제공하고 중국어와 같은 조 언어에서 단어의 의미를 정의하는 데 도움을 줍니다. 피치는 본질적으로 파형의 주기성 또는 반복율의 지각적 상관관계입니다: 파형이 시간이 지남에 따라 반복되는 속도가 빠를수록 인지되는 피치가 높아집니다. 가장 일반적인 피치를 연상시키는 소리는 고조파 복합 톤으로 알려져 있습니다. 고조파 복소음은 두 개 이상의 주파수로 구성되기 때문에 복잡하고, 주파수가 모두 공통 기본 주파수(F0)의 정수 배수이기 때문에 고조파입니다. 예를 들어, F0가 100Hz인 고조파 복합 톤은 200, 300, 400Hz 등의 주파수에서도 에너지를 포함합니다. 이러한 높은 주파수는 고조파 또는 배음으로 알려져 있으며, 소리의 높낮이를 결정하는 데 중요한 역할을 합니다. 실제로 F0의 에너지가 없거나 가려져 있어도 일반적으로 나머지 소리는 F0에 해당하는 피치를 갖는 것으로 인식합니다. 이 현상을 '누락된 기본음의 음정'이라고 하며, 음정에 관한 이론과 모델을 형성하는 데 중요한 역할을 했습니다(de Cheveigné, 2005). 우리는 약 30Hz(Pressnitzer, Patterson, &amp; Krumbholz, 2001)에서 최대 약 4~5kHz(Attneave &amp; Olson, 1971; Oxenham, Micheyl, Keebler, Loper, &amp; Santurette, 2011)에 이르는 F0 범위에서 멜로디를 인식하기에 충분한 정확도로 피치를 들을 수 있습니다. 이 범위는 악기의 음역과도 상당히 일치하는데, 예를 들어 현대 그랜드 피아노의 음역은 27.5Hz에서 4,186Hz에 이릅니다. 인간은 5,000Hz 이상의 주파수 변화를 구분할 수 있지만 멜로디를 인식하거나 음악 간격을 판단하는 데는 더 이상 정확하지 않습니다.</p><h2 id="timbre">음색</h2><p>음색은 소리의 품질을 의미하며, 흔히 밝다, 둔하다, 거칠다, 공허하다 등의 단어로 묘사됩니다. 엄밀히 말하면 음색은 음량, 음정, 지속 시간이 같은 두 소리를 구분할 수 있는 모든 것을 포함합니다. 예를 들어, 같은 음을 연주하는 바이올린과 피아노는 음질이나 음색에 따라 매우 다르게 들립니다.<br><br>음색의 중요한 측면은 소리의 스펙트럼 내용입니다. 고주파 에너지가 더 많은 소리는 저주파 에너지가 더 많은 소리보다 더 밝거나, 더 작거나, 더 거칠게 들리는 경향이 있으며, 깊거나, 풍부하거나, 둔한 소리로 표현될 수 있습니다. 음색의 다른 중요한 측면으로는 소리의 시간적 범위(또는 윤곽), 특히 소리의 시작과 끝이 어떻게 되는지가 있습니다. 예를 들어 피아노는 해머가 현을 치면서 빠르게 시작되는 반면, 클라리넷 음표의 어택은 훨씬 더 점진적일 수 있습니다. 예를 들어 녹음을 거꾸로 재생하는 등 피아노 음의 시작 부분을 인위적으로 변경하면 음의 성격이 크게 바뀌어 더 이상 피아노 음으로 인식할 수 없게 될 수 있습니다. 일반적으로 전체 스펙트럼 내용과 시간적 엔벨로프는 모든 소리에 대한 좋은 첫 번째 근사치를 제공할 수 있지만, 시간에 따른 스펙트럼의 미묘한 변화(또는 스펙트로-시간적 변화)는 자연스러운 악기의 모방을 만드는 데 중요한 역할을 합니다(Risset &amp; Wessel, 1999).</p><h1 id="an-overview-of-the-auditory-system">청각 시스템 개요</h1><figure data-align="full"><img loading="lazy" title="Diagram of the outer, middle, and inner ear." src="https://nobaproject.com/images/shared/images/000/000/075/original.png" alt="Diagram of the outer, middle, and inner ear." data-is-external-image="true"><figcaption>Figure 1: Diagram of the outer, middle, and inner ear.</figcaption></figure><p>우리의 청각적 지각은 귀를 통해 소리가 처리되는 방식에 따라 달라집니다. 귀는 외이, 중이, 내이의 세 가지 주요 부분으로 나눌 수 있습니다(그림 1 참조). 외이는 귓바퀴(귀의 눈에 보이는 부분으로 독특한 주름과 돌기가 있는 부분), 외이도(또는 청각 근육), 고막으로 구성됩니다. 물론 우리 대부분은 두 개의 귀를 가지고 있는데, 이는 소리가 어디에서 나는지 알아내려고 할 때 특히 유용합니다. 아래 공간 청각 섹션에서 설명하는 것처럼, 우리의 뇌는 두 귀에서 나오는 신호의 미묘한 차이를 비교하여 공간에서 소리를 찾아낼 수 있습니다. 하지만 이 방법이 항상 도움이 되는 것은 아닙니다. 예를 들어, 바로 앞이나 바로 뒤에서 들리는 소리는 양쪽 귀에 차이를 만들지 않습니다. 이러한 경우 귓바퀴에서 생성되는 필터링은 소리를 찾아내고 잠재적인 앞뒤 및 위아래 혼동을 해결하는 데 도움이 됩니다. 보다 일반적으로, 고막의 주름과 돌기는 음원의 위치에 따라 주파수 응답에서 뚜렷한 최고점과 최저점을 생성합니다. 그러면 뇌는 특정 패턴의 스펙트럼 피크와 딥을 특정 공간적 위치와 연관시키는 방법을 학습합니다. 흥미롭게도 이렇게 학습된 연관성은 성인이 된 후에도 가소성, 즉 가변성을 유지합니다. 예를 들어, 몰드를 사용하여 귓바퀴를 변형시킨 한 연구에 따르면 사람들은 몇 주 만에 "새로운" 귀를 정확하게 사용하는 방법을 배울 수 있었습니다(Hofman, Van Riswick, &amp; Van Opstal, 1998). 귓바퀴의 크기가 작기 때문에 이러한 종류의 음향 단서는 약 2kHz 이상의 고주파수에서만 발견됩니다. 저주파수에서는 소리가 위, 앞, 아래에서 들리든 기본적으로 변하지 않습니다. 외이도는 약 1~4kHz 영역의 소리를 증폭하는 데 도움이 되는 관으로, 특히 음성 의사소통에 중요한 영역입니다.<br><br>중이는 공기로 채워진 공간으로 구성되어 있으며, 이 공간에는 각각의 모양에 따라 귓바퀴, 망치, 등자 또는 모루, 망치, 등자라고 알려진 중이 뼈가 있습니다. 이 뼈는 신체에서 가장 작은 뼈라는 특징이 있습니다. 고막의 주요 기능은 고막의 진동을 달팽이관의 타원형 창으로 전달하고, 일종의 지렛대 작용을 통해 고막을 둘러싼 공기의 임피던스와 달팽이관 내 액체의 임피던스를 더 잘 일치시키는 것입니다.<br><br>내이에는 두개골의 측두골에 둘러싸인 달팽이관이 있으며, 달팽이관에서는 소리의 기계적 진동이 뇌에서 처리되는 신경 신호로 변환됩니다. 달팽이관은 체액으로 채워진 나선형 구조입니다. 나선의 길이를 따라 기저막이 있으며, 기저막은 타원형 창문의 진동에 의해 발생하는 압력 차이에 반응하여 진동합니다. 기저막 위에 있는 코르티 기관은 기저막의 기저부(타원형 창 옆)에서 나선의 정점(나선의 "끝")까지 전체 길이에 걸쳐 있습니다. 코르티 기관은 세 줄의 바깥쪽 유모세포와 한 줄의 안쪽 유모세포로 구성되어 있습니다. 유모세포는 작은 털, 즉 세모세포를 통해 진동을 감지합니다. 바깥쪽 유모세포는 소리로 인한 진동을 기계적으로 증폭하는 기능을 하는 반면, 안쪽 유모세포는 청신경과 시냅스를 형성하고 이러한 진동을 활동 전위 또는 신경 스파이크로 변환하여 청신경을 따라 청각 경로의 더 높은 중추로 전달합니다.</p><p>청각의 가장 중요한 원리 중 하나인 주파수 분석은 달팽이관에서 이루어집니다. 달팽이관의 작용은 프리즘의 작용에 비유할 수 있는데, 복잡한 소리를 구성하는 많은 주파수를 구성 주파수로 분해하여 저주파는 달팽이관 정점 근처에서 최대 기저막 진동을 일으키고 고주파는 달팽이관 기저부 근처에서 최대 기저막 진동을 일으킵니다. 이렇게 소리를 구성 주파수로 분해하고 주파수 간 매핑 또는 "토노토픽" 표현을 하는 것은 청각 시스템의 주요 구성 원리이며 달팽이관에서 일차 청각 피질에 이르기까지 소리의 신경적 표현에서 유지됩니다. 소리를 구성 주파수 성분으로 분해하는 것은 우리가 한 번에 두 개 이상의 소리를 들을 수 있는 원리의 일부입니다. 달팽이관 내의 여기 위치로 주파수를 나타내는 것 외에도 주파수는 청신경 내의 스파이크 타이밍으로 표현되기도 합니다. "위상 고정"으로 알려진 이 속성은 두 귀 사이의 파형 도착 시간 차이를 비교하는 데 매우 중요합니다(아래 공간 청각 섹션 참조).<br><br>일차 시각 피질(또는 V1)이 초기 처리 단계로 간주되는 시각과 달리, 청각 신호는 측두엽에 위치한 일차 청각 피질에 도달하기 전에 여러 단계의 처리 단계를 거칩니다. 달팽이관의 전기기계적 특성과 다양한 구조에 대한 이해는 상당히 높지만, 청각 경로의 상위 단계에서 이루어지는 처리에 대한 이해는 다소 초보적인 수준에 머물러 있습니다. 공간적 국소화와 공간의 특정 위치에 맞춰진 뉴런을 제외하면(Harper &amp; McAlpine, 2004; Knudsen &amp; Konishi, 1978), 청각적 특징 추출 및 표현의 방법, 내용, 위치에 대한 합의가 거의 이루어지지 않고 있습니다. 청각 피질에 '피치 센터'가 있다는 증거는 인간 신경 영상 연구(예: Griffiths, Buchel, Frackowiak, &amp; Patterson, 1998; Penagos, Melcher, &amp; Oxenham, 2004)와 단일 단위 생리학 연구(Bendor &amp; Wang, 2005)에서 모두 발견되었지만, 여기에서도 피질의 단일 영역이 피치와 같은 단일 특징을 코딩하는지 또는 코드가 더 분산되어 있는지(Walker, Bizley, King, &amp; Schnupp, 2011)와 관련하여 여전히 몇 가지 질문이 남아있습니다.</p><h1 id="audibility-masking-and-frequency-selectivity">가청도, 마스킹 및 주파수 선택성</h1><p>전반적으로 인간의 달팽이관은 매우 넓은 범위의 주파수에 걸쳐 청각을 제공합니다. 정상적인 청력을 가진 젊은 사람들은 약 20Hz에서 최대 20kHz에 이르는 주파수의 소리를 인식할 수 있습니다. 우리가 감지할 수 있는 강도의 범위도 인상적입니다. 중간 주파수 범위(약 1~4kHz)에서 들을 수 있는 가장 조용한 소리는 급격하고 영구적인 청력 손실 없이 들을 수 있는 가장 큰 소리보다 약 1,000,000,000,000배 낮은 강도의 소리를 냅니다. 이 엄청난 다이나믹 레인지 때문에 음압 또는 강도를 설명할 때 데시벨(dB)이라는 로그 눈금을 사용하는 경향이 있습니다. 이 눈금에서 0dB 음압 레벨(SPL)은 20 마이크로파스칼(μPa)로 정의되며, 이는 대략 가장 조용한 지각 가능한 소음 수준에 해당하며 120dB SPL은 위험할 정도로 큰 소리로 간주됩니다.</p><figure data-align="left"><img loading="lazy" title="A group of motor scooters and a bicyclist wait at an intersection for the traffic light to change." src="https://nobaproject.com/images/shared/images/000/002/610/original.jpg" alt="A group of motor scooters and a bicyclist wait at an intersection for the traffic light to change." data-is-external-image="true"><figcaption>When the frequency content of different sounds overlaps, masking occurs. Less intense sounds become difficult or impossible to hear because more intense sounds dominate and interfere. Crowded restaurants or busy city streets full of traffic are typical examples of places where certain sounds can "swamp" others. [Image: Peter van der Sluijs, https://goo.gl/K8L4c0, CC BY-SA 3.0, https://goo.gl/eLCn2O]</figcaption></figure><p>마스킹은 한 소리의 존재가 다른 소리를 더 잘 듣지 못하게 만드는 현상입니다. 샤워하는 동안 전화 벨소리를 듣지 못하거나 시끄러운 식당에서 대화를 따라가기 어려울 때 등 일상 생활에서 누구나 마스킹을 경험하게 됩니다. 일반적으로 특정 조건이 충족되면 더 강한 소리가 덜 강한 소리를 가릴 수 있습니다. 가장 중요한 조건은 소리의 주파수 내용이 겹쳐서 마스킹 소리에 의해 생성된 달팽이관의 활동이 대상 소리에 의해 생성된 달팽이관의 활동을 "늪"으로 몰아넣는다는 것입니다. "억제"로 알려진 또 다른 유형의 마스킹은 마스커에 대한 반응이 목표 소리에 대한 신경(및 경우에 따라 기계적) 반응을 감소시킬 때 발생합니다. 달팽이관의 필터링 기능으로 인해 저주파 사운드는 고주파 사운드를 마스킹할 가능성이 높으며, 특히 높은 사운드 강도에서는 그 반대의 경우가 더 많습니다. 이러한 마스킹의 비대칭적인 측면을 "마스킹의 상향 확산"이라고 합니다. 달팽이관 손상에 수반되는 날카로운 달팽이관 조율의 손실은 더 넓은 필터링과 더 많은 마스킹으로 이어지며, 이는 시끄러운 환경에서 청력 손실이 있는 사람들이 겪는 어려움에 기여할 수 있는 생리적 현상입니다(Moore, 2007).<br><br>많은 마스킹이 달팽이관 내 상호작용으로 설명될 수 있지만, 그렇게 쉽게 설명할 수 없고 달팽이관 내 상호작용이 거의 없을 때에도 발생할 수 있는 다른 형태가 있습니다. 이러한 보다 중심적인 형태의 마스킹은 다양한 형태로 나타나지만 종종 "정보 마스킹"이라는 용어로 함께 분류됩니다(Durlach et al., 2003; Watson &amp; Kelly, 1978). 정보 마스킹의 원인에 대해서는 알려진 바가 거의 없지만, 대부분의 형태는 마스커와 표적 소리의 지각적 "융합" 또는 적어도 마스킹 소리에서 표적을 분리하지 못하기 때문일 수 있습니다. 또한 정보 마스킹의 생리적 위치에 대해서는 알려진 바가 거의 없지만, 적어도 일부 형태는 청각 피질에서 시작되는 것으로 보인다는 점을 제외하면 상대적으로 거의 알려지지 않았습니다(Gutschalk, Micheyl, &amp; Oxenham, 2008).</p><h1 id="spatial-hearing">공간 청각</h1><p>시각과 달리 청각은 360°의 시야를 가지고 있습니다. 하지만 청각은 공간에서 물체의 위치를 파악하는 데 시각보다 적어도 몇 배는 떨어집니다. 따라서 청각적 위치 파악 능력은 우리에게 경고를 보내고 방향을 잡는 데 가장 유용하며, 시각은 일반적으로 더 세밀한 분석을 제공합니다. 물론 종마다 차이가 있으며, 수리부엉이나 반향 탐지 박쥐와 같은 일부 동물은 고도로 전문화된 소리 위치 파악 시스템을 개발했습니다.</p><figure data-align="right"><img loading="lazy" title="A man puts his hand behind his unusually large ear as if trying to locate the location of a sound." src="https://nobaproject.com/images/shared/images/000/002/611/original.jpg" alt="A man puts his hand behind his unusually large ear as if trying to locate the location of a sound." data-is-external-image="true"><figcaption>Humans are able to locate sound in space to determine whether the source is in front of us or behind us, or whether it is elevated or below us. [Photo: David Goehring, https://goo.gl/UOLZpB, CC BY 2.0, https://goo.gl/BRvSA7]</figcaption></figure><p>공간에서 음원을 찾아내는 인간의 능력은 신경 계산의 놀라운 업적입니다. 두 가지 주요 정보 소스는 모두 양쪽 귀에서 들리는 소리를 비교하는 데서 비롯됩니다. 첫 번째는 귀간 시간차(ITD)를 기반으로 하며, 왼쪽의 음원이 오른쪽 귀에 도달하기 약간 전에 왼쪽 귀에 도달하는 소리를 발생시킨다는 사실에 의존합니다. 소리는 빛보다 훨씬 느리지만, 그 속도는 여전히 두 귀 사이의 도착 시간이 밀리초 단위로 차이가 난다는 것을 의미합니다. 현실 세계에서 우리가 접하는 가장 큰 ITD(소리가 바로 왼쪽이나 오른쪽에 있을 때)는 0.5밀리초가 조금 넘는 정도에 불과합니다. 약간의 연습을 통해 인간은 10~20μs(즉, 2천만 분의 1초)의 ITD를 감지하는 방법을 배울 수 있습니다(Klump &amp; Eady, 1956).<br><br>두 번째 정보 소스는 귀간 레벨 차이(ILD)에 기반합니다. 고주파수(약 1kHz 이상)에서는 머리에 음향 '그림자'가 드리워져 왼쪽에서 소리가 들릴 때 왼쪽 귀의 음량이 오른쪽 귀의 음량보다 다소 높아집니다. 매우 높은 주파수에서 ILD는 20dB에 달할 수 있으며, 우리는 1dB 정도의 작은 차이에도 민감하게 반응합니다.<br><br>외이에 대한 설명에서 잠깐 언급했듯이, 음원의 고도 또는 음원이 앞쪽에서 오는지 뒤에서 오는지에 관한 정보는 귓바퀴의 필터링 효과로 인해 발생하는 고주파 스펙트럼 세부 정보에 포함되어 있습니다.<br><br>일반적으로 우리는 저주파수(약 1.5kHz 이하)에서 ITD에 가장 민감합니다. 더 높은 주파수에서도 우리는 소리의 천천히 변화하는 시간적 엔벨로프에 기반한 타이밍의 변화를 감지할 수 있지만 시간적 미세 구조는 감지하지 못하는데, 이는 아마도 고주파에서 시간적 미세 구조에 대한 신경 위상 고정이 손실되기 때문일 수 있습니다(Bernstein &amp; Trahiotis, 2002; Smith, Delgutte, &amp; Oxenham, 2002). 이와는 대조적으로, ILD는 헤드 섀도우가 가장 큰 고주파수에서 가장 유용합니다. 이렇게 서로 다른 주파수 영역에서 서로 다른 음향 단서를 사용함으로써 소리의 위치 파악에 대한 고전적이고 초기의 "이중 이론"이 탄생했습니다(Rayleigh, 1907). 주파수 스펙트럼이 넓은 일상적인 소리의 경우, 공간적 위치에 대한 우리의 지각은 저주파 시간적 미세 구조의 귓바퀴 간 시간차에 의해 지배되는 것으로 보입니다(Macpherson &amp; Middlebrooks, 2002).<br><br>시각과 마찬가지로 거리에 대한 우리의 지각은 상황에 따라 크게 달라집니다. 누군가가 매우 낮은 소리로 소리를 지르는 것을 들으면, 우리는 소리 지르는 사람의 소리 속성에 대한 지식을 바탕으로 소리 지르는 사람이 멀리 떨어져 있을 것이라고 추론합니다. 방이나 기타 밀폐된 장소에서는 잔향이 거리에 대한 정보를 제공할 수도 있습니다: 화자가 멀어질수록 직접 음량은 감소하지만 잔향의 음량은 거의 동일하게 유지되므로 직접 대 잔향 에너지의 비율은 감소합니다(Zahorik &amp; Wightman, 2001).</p><h1 id="auditory-scene-analysis">청각 장면 분석</h1><p>카페에서 배경 음악이 흘러나오고 카운터 뒤에서 커피잔이 덜컹거리는 소리, 밖에서 차가 다니는 소리, 옆 테이블에서 대화가 진행되는 등 주변 환경에는 보통 한 번에 두 가지 이상의 음원이 존재합니다. 이러한 모든 소스는 고막에서 하나의 복잡한 파형을 형성하기 위해 결합된 음파를 생성하며, 그 모양은 개별 음원에서 생성되는 파형과 거의 관계가 없을 수 있습니다. 청각 시스템은 어떻게든 이러한 복잡한 파형을 분해하거나 분해할 수 있으며, 시간이 지남에 따라 소리가 펼쳐질 때 따라갈 수 있는 별도의 청각 '객체' 또는 '흐름'을 형성하여 음향 환경을 이해할 수 있게 해줍니다(Bregman, 1990).<br><br>소리 요소를 그룹화하여 하나의 객체를 형성하거나 분리하여 여러 객체를 형성하는 방법을 설명하기 위해 여러 가지 휴리스틱 원칙이 공식화되었습니다. 이러한 원리 중 다수는 막스 베르트하이머와 같은 이른바 게슈탈트 심리학자들이 시각에 대해 제안한 초기 아이디어에서 비롯되었습니다. 이러한 경험 법칙에 따르면 시간이나 주파수 면에서 가까운 거리에 있는 소리는 함께 그룹화되는 경향이 있습니다. 또한 동시에 시작하고 끝나는 소리는 하나의 청각적 대상을 형성하는 경향이 있습니다. 흥미롭게도 공간적 위치가 항상 강력하거나 신뢰할 수 있는 그룹화 단서가 되는 것은 아닌데, 이는 개별 주파수 성분의 위치 정보가 반향의 영향으로 인해 모호한 경우가 많기 때문일 수 있습니다. 여러 연구에서 한 단서를 다른 단서와 '트레이드 오프'하여 서로 다른 단서의 상대적 중요도를 조사했습니다. 일부 연구에서는 양쪽 귀에 제시된 소리에 존재하지 않는 멜로디가 지각에 나타나거나(Deutsch, 1979), 경쟁하는 지각 조직에서 소리 요소가 지각적으로 "손실"되는 흥미로운 청각 착시를 발견하기도 했습니다(Shinn-Cunningham, Lee, &amp; Oxenham, 2007).<br><br>최근의 시도는 청각 장면 분석의 메커니즘을 밝히기 위해 계산적이고 중립적인 접근 방식을 사용했으며(예: Elhilali, Ma, Micheyl, Oxenham, &amp; Shamma, 2009), 복잡한 청각 장면의 구문 분석과 지각을 이해하기 위해 보다 원칙적이고 덜 휴리스틱적인 접근 방식으로 나아가기 위한 노력으로 계산 청각 장면 분석(CASA) 분야가 부분적으로 등장했습니다(예: Wang &amp; Brown, 2006). 이 문제를 해결하면 인간의 청각 지각을 더 잘 이해할 수 있을 뿐만 아니라 "스마트" 보청기와 인공와우, 그리고 배경 소음에 더 강한 자동 음성 인식 시스템에 대한 새로운 접근 방식을 제공할 수 있습니다.</p><h1 id="conclusion">결론</h1><figure data-align="left"><img loading="lazy" title="An infant with a cochlear implant." src="https://nobaproject.com/images/shared/images/000/002/612/original.jpg" alt="An infant with a cochlear implant." data-is-external-image="true"><figcaption>An infant with a cochlear implant. [Image: Bjorn Knetsch, https://goo.gl/J2wCvJ, CC BY 2.0, https://goo.gl/BRvSA7]</figcaption></figure><p>청각은 주변 사람들과의 가장 중요한 연결고리를 제공합니다. 청각 시스템의 복잡한 생리학은 귀에 도달하는 기압의 미세한 변화를 음성, 음악, 주변 환경의 소리로 인식하는 방대한 청각적 경험으로 변환합니다. 우리는 청각 시스템의 상위 단계에서 신경 코딩의 기본 원리와 그것이 지각과 어떻게 관련되는지 이제 막 이해하기 시작했습니다. 하지만 청력 손실이 심한 사람들을 위해 귀의 일부 기능을 재현하는 인공와우와 같은 장치를 통해 수십만 명의 삶이 개선되었습니다.</p></section><section><h2 id="outside-resources">Outside Resources</h2><dl class="noba-chapter-resources"><dt>Audio: Auditory Demonstrations from Richard Warren’s lab at the University of Wisconsin, Milwaukee</dt><dd><a href="http://www4.uwm.edu/APL/demonstrations.html">http://www4.uwm.edu/APL/demonstrations.html</a></dd><dt>Audio: Auditory Demonstrations. CD published by the Acoustical Society of America (ASA). You can listen to the demonstrations here</dt><dd><a href="http://www.feilding.net/sfuad/musi3012-01/demos/audio/">http://www.feilding.net/sfuad/musi3012-01/demos/audio/</a></dd><dt>Web: Demonstrations and illustrations of cochlear mechanics can be found here</dt><dd><a href="http://lab.rockefeller.edu/hudspeth/graphicalSimulations">http://lab.rockefeller.edu/hudspeth/graphicalSimulations</a></dd><dt>Web: More demonstrations and illustrations of cochlear mechanics</dt><dd><a href="http://www.neurophys.wisc.edu/animations/">http://www.neurophys.wisc.edu/animations/</a></dd></dl></section><section><h2 id="discussion-questions">Discussion Questions</h2><ol><li>Based on the available acoustic cues, how good do you think we are at judging whether a low-frequency sound is coming from in front of us or behind us? How might we solve this problem in the real world?</li><li>Outer hair cells contribute not only to amplification but also to the frequency tuning in the cochlea. What are some of the difficulties that might arise for people with cochlear hearing loss, due to these two factors? Why do hearing aids not solve all these problems?</li><li>Why do you think the auditory system has so many stages of processing before the signals reach the auditory cortex, compared to the visual system? Is there a difference in the speed of processing required?</li></ol></section><section><h2 id="vocabulary">Vocabulary</h2><dl class="noba-chapter-vocabulary"><dt id="vocabulary-cochlea" data-term="cochlea">Cochlea</dt><dd>Snail-shell-shaped organ that transduces mechanical vibrations into neural signals.</dd><dt id="vocabulary-interaural-differences" data-term="interaural-differences">Interaural differences</dt><dd>Differences (usually in time or intensity) between the two ears.</dd><dt id="vocabulary-pinna" data-term="pinna">Pinna</dt><dd>Visible part of the outer ear.</dd><dt id="vocabulary-tympanic-membrane" data-term="tympanic-membrane">Tympanic membrane</dt><dd>Ear drum, which separates the outer ear from the middle ear.</dd></dl></section><section><h2 id="references">References</h2><ul class="noba-chapter-references"><li id="reference-1" data-reference="1">Attneave, F., &amp; Olson, R. K. (1971). Pitch as a medium: A new approach to psychophysical scaling. <em>American Journal of Psychology</em>, 84, 147–166.</li><li id="reference-2" data-reference="2">Bendor, D., &amp; Wang, X. (2005). The neuronal representation of pitch in primate auditory cortex. <em>Nature</em>, 436, 1161–1165.</li><li id="reference-3" data-reference="3">Bernstein, L. R., &amp; Trahiotis, C. (2002). Enhancing sensitivity to interaural delays at high frequencies by using "transposed stimuli." <em>Journal of the Acoustical Society of America</em>, 112, 1026–1036.</li><li id="reference-4" data-reference="4">Bregman, A. S. (1990). <em>Auditory scene analysis: The perceptual organization of sound</em>. Cambridge, MA: MIT Press.</li><li id="reference-6" data-reference="6">Deutsch, D. (1979). Binaural integration of melodic patterns. <em>Perception &amp; Psychophysics</em>, 25, 399–405.</li><li id="reference-7" data-reference="7">Durlach, N. I., Mason, C. R., Kidd, G., Jr., Arbogast, T. L., Colburn, H. S., &amp; Shinn-Cunningham, B. G. (2003). Note on informational masking. <em>Journal of the Acoustical Society of America</em>, 113, 2984–2987.</li><li id="reference-8" data-reference="8">Elhilali, M., Ma, L., Micheyl, C., Oxenham, A. J., &amp; Shamma, S. (2009). Temporal coherence in the perceptual organization and cortical representation of auditory scenes. <em>Neuron</em>, 61, 317–329.</li><li id="reference-9" data-reference="9">Fechner, G. T. (1860). <em>Elemente der Psychophysik</em> (Vol. 1). Leipzig, Germany: Breitkopf und Haertl.</li><li id="reference-10" data-reference="10">Griffiths, T. D., Buchel, C., Frackowiak, R. S., &amp; Patterson, R. D. (1998). Analysis of temporal structure in sound by the human brain. <em>Nature Neuroscience</em>, 1, 422–427.</li><li id="reference-11" data-reference="11">Gutschalk, A., Micheyl, C., &amp; Oxenham, A. J. (2008). Neural correlates of auditory perceptual awareness under informational masking. <em>PLoS Biology</em>, 6, 1156–1165 (e1138).</li><li id="reference-12" data-reference="12">Harper, N. S., &amp; McAlpine, D. (2004). Optimal neural population coding of an auditory spatial cue. <em>Nature</em>, 430, 682–686.</li><li id="reference-13" data-reference="13">Hofman, P. M., Van Riswick, J. G. A., &amp; Van Opstal, A. J. (1998). Relearning sound localization with new ears. <em>Nature Neuroscience</em>, 1, 417–421.</li><li id="reference-14" data-reference="14">ISO. (2003). <em>ISO:226 Acoustics - Normal equal-loudness-level contours</em>. Geneva, Switzerland: International Organization for Standardization.</li><li id="reference-15" data-reference="15">Klump, R. G., &amp; Eady, H. R. (1956). Some measurements of interaural time difference thresholds. <em>Journal of the Acoustical Society of America</em>, 28, 859–860.</li><li id="reference-16" data-reference="16">Knudsen, E. I., &amp; Konishi, M. (1978). A neural map of auditory space in the owl. <em>Science</em>, 200, 795–797.</li><li id="reference-17" data-reference="17">Macpherson, E. A., &amp; Middlebrooks, J. C. (2002). Listener weighting of cues for lateral angle: The duplex theory of sound localization revisited. <em>Journal of the Acoustical Society of America</em>, 111, 2219–2236.</li><li id="reference-18" data-reference="18">Moore, B. C. J. (2007). <em>Cochlear hearing loss: Physiological, psychological, and technical issues</em>. Chichester: Wiley.</li><li id="reference-19" data-reference="19">Moore, B. C. J., Glasberg, B. R., &amp; Baer, T. (1997). A model for the prediction of thresholds, loudness, and partial loudness. <em>Journal of the Audio Engineering Society</em>, 45, 224–240.</li><li id="reference-20" data-reference="20">Oxenham, A. J., Micheyl, C., Keebler, M. V., Loper, A., &amp; Santurette, S. (2011). Pitch perception beyond the traditional existence region of pitch. <em>Proceedings of the National Academy of Sciences USA</em>, 108, 7629–7634.</li><li id="reference-22" data-reference="22">Penagos, H., Melcher, J. R., &amp; Oxenham, A. J. (2004). A neural representation of pitch salience in non-primary human auditory cortex revealed with fMRI. <em>Journal of Neuroscience</em>, 24, 6810–6815.</li><li id="reference-23" data-reference="23">Pressnitzer, D., Patterson, R. D., &amp; Krumbholz, K. (2001). The lower limit of melodic pitch. <em>Journal of the Acoustical Society of America</em>, 109, 2074–2084.</li><li id="reference-24" data-reference="24">Rayleigh, L. (1907). On our perception of sound direction. <em>Philosophical Magazine</em>, 13, 214–232.</li><li id="reference-25" data-reference="25">Risset, J. C., &amp; Wessel, D. L. (1999). Exploration of timbre by analysis and synthesis. In D. Deutsch (Ed.), <em>The psychology of music</em> (2nd ed., pp. 113–168): Academic Press.</li><li id="reference-26" data-reference="26">Shinn-Cunningham, B. G., Lee, A. K., &amp; Oxenham, A. J. (2007). A sound element gets lost in perceptual competition. <em>Proceedings of the National Academy of Sciences USA</em>, 104, 12223–12227.</li><li id="reference-27" data-reference="27">Smith, Z. M., Delgutte, B., &amp; Oxenham, A. J. (2002). Chimaeric sounds reveal dichotomies in auditory perception. <em>Nature</em>, 416, 87–90.</li><li id="reference-28" data-reference="28">Stevens, S. S. (1957). On the psychophysical law. <em>Psychological Review</em>, 64, 153–181.</li><li id="reference-29" data-reference="29">Walker, K. M., Bizley, J. K., King, A. J., &amp; Schnupp, J. W. (2011). Multiplexed and robust representations of sound features in auditory cortex. <em>Journal of Neuroscience</em>, 31, 14565–14576.</li><li id="reference-30" data-reference="30">Wang, D., &amp; Brown, G. J. (Eds.). (2006). <em>Computational auditory scene analysis: Principles, algorithms, and applications</em>. Hoboken, NJ: Wiley.</li><li id="reference-31" data-reference="31">Watson, C. S., &amp; Kelly, W. J. (1978). Informational masking in auditory patterns. <em>Journal of the Acoustical Society of America</em>, 64, S39.</li><li id="reference-32" data-reference="32">Zahorik, P., &amp; Wightman, F. L. (2001). Loudness constancy with varying sound source distance. <em>Nature Neuroscience</em>, 4, 78–83.</li><li id="reference-5" data-reference="5">de Cheveigné, A. (2005). Pitch perception models. In C. J. Plack, A. J. Oxenham, A. N. Popper, &amp; R. Fay (Eds.), <em>Pitch: Neural coding and perception</em> (pp. 169–233). New York, NY: Springer Verlag.</li></ul></section><section><h2 id="authors">Authors</h2><ul class="media-list"><li class="media"><figure class="media-object noba-author pull-right"><img loading="lazy" src="https://nobaproject.com/images/shared/author_photos/000/000/065/large.jpg" alt="" width="150" height="150" data-is-external-image="true"></figure><div class="media-body"><div class="media-heading">Andrew J. Oxenham</div>Andrew Oxenham is a Distinguished McKnight University Professor in the Departments of Psychology and Otolaryngology at the University of Minnesota. He has published over 100 peer-reviewed papers in the field of auditory perception, and has received major awards from the Acoustical Society of America and the National Academy of Sciences.</div></li></ul></section><section><h2 id="license">Creative Commons License</h2><small class="license"><a class="marks" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en_US"><img title="Creative Commons" src="https://nobaproject.com/assets/licensing/cc-7e377801d36ddb6d62c1c06dd07858f400efd7284459955e0de47bdb796c8658.png" alt="Creative Commons" data-is-external-image="true"><img loading="lazy" title="Attribution" src="https://nobaproject.com/assets/licensing/by-9be0271defac0fba0df496e1e35b7cd2aeaed8630b22b935ce2ea51380c98cba.png" alt="Attribution" data-is-external-image="true"><img title="Non-Commerical" src="https://nobaproject.com/assets/licensing/nc-1f33b73ce264f326ba55092ac717ed56b21800b76bbd849859eacf7d9319745f.png" alt="Non-Commerical" data-is-external-image="true"><img loading="lazy" title="Share-Alike" src="https://nobaproject.com/assets/licensing/sa-1725398b2ebf51d6d0165a63b36061120a047cceed2a5be57cf3f99ad65c3668.png" alt="Share-Alike" data-is-external-image="true"></a><span class="title">Hearing</span> by <a href="https://nobaproject.com/modules/hearing#authors" property="cc:attributionName" rel="cc:attributionURL" xmlns:cc="https://creativecommons.org/ns#">Andrew J. Oxenham</a> is licensed under a <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en_US" rel="license">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>. Permissions beyond the scope of this license may be available in our <a href="https://nobaproject.com/license-agreement" rel="cc:morePermissions" xmlns:cc="https://creativecommons.org/ns">Licensing Agreement</a>.</small></section><section><h2 id="apa">How to cite this Noba module using APA Style</h2>Oxenham, A. J. (2023). Hearing. In R. Biswas-Diener &amp; E. Diener (Eds), <i>Noba textbook series: Psychology.</i> Champaign, IL: DEF publishers. Retrieved from <a href="http://noba.to/jry3cu78">http://noba.to/jry3cu78</a></section></article></div><footer class="post__inner post__footer"><p class="post__last-updated">This article was updated on 7월 3, 2023</p><div class="post__share-tag-container"><div class="post__tag"><h3>Tagged in:</h3><ul><li><a href="https://learningjunho.github.io/noba-ko/tags/gamgaggwa-jigagsensation-and-perception/">감각과 지각(Sensation and Perception)</a></li></ul></div><div class="post__share"><button class="post__share-button js-post__share-button icon" aria-label="Share button"><i class="fas fa-share-alt"></i></button><div class="post__share-popup js-post__share-popup"></div></div></div><div class="post__bio"><div><h3><a href="https://learningjunho.github.io/noba-ko/authors/learningjunho/" class="invert" rel="author">learningjunho</a></h3><p>심리학을 공부하기 위해 noba project를 한국어로 번역하는 도전을 시작했습니다. 번역에 DeepL과 GPT-4를 적극적으로 활용했습니다.</p></div></div></footer></article></main><footer id="copyright"><ul><li>© Massively</li><li>Design: <a href="https://html5up.net" target="_blank" rel="nofollow noopener">HTML5 UP</a></li><li>Powered by Publii</li><li>learningjunho 번역</li></ul></footer></div><script src="https://learningjunho.github.io/noba-ko/assets/js/jquery.min.js?v=7c14a783dfeb3d238ccd3edd840d82ee"></script><script src="https://learningjunho.github.io/noba-ko/assets/js/jquery.scrollex.min.js?v=f89065e3d988006af9791b44561d7c90"></script><script src="https://learningjunho.github.io/noba-ko/assets/js/jquery.scrolly.min.js?v=1ed5a78bde1476875a40f6b9ff44fc14"></script><script src="https://learningjunho.github.io/noba-ko/assets/js/browser.min.js?v=c07298dd19048a8a69ad97e754dfe8d0"></script><script src="https://learningjunho.github.io/noba-ko/assets/js/breakpoints.min.js?v=81a479eb099e3b187613943b085923b8"></script><script src="https://learningjunho.github.io/noba-ko/assets/js/util.min.js?v=4201a626f8c9b614a663b3a1d7d82615"></script><script src="https://learningjunho.github.io/noba-ko/assets/js/main.min.js?v=56233c354bd814758be8bff42f7e13a5"></script><script>/*<![CDATA[*/var images=document.querySelectorAll("img[loading]");for(var i=0;i<images.length;i++){if(images[i].complete){images[i].classList.add("is-loaded")}else{images[i].addEventListener("load",function(){this.classList.add("is-loaded")},false)}};/*]]>*/</script></body></html>