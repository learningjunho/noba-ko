<!DOCTYPE html><html lang="ko"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><title>멀티모델 지각(Multi-Modal Perception) - noba-ko</title><meta name="description" content="멀티모델 지각 By Lorin Lachs California State University, Fresno 대부분의 경우, 우리는 세상을 여러 감각 양식에서 나오는 감각의 통합된 묶음으로 인식합니다. 다시 말해,&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><script type="text/javascript" async src="https://www.googletagmanager.com/gtag/js?id=G-QFEP3D8PNE"></script><script type="text/javascript">window.dataLayer = window.dataLayer || [];
				  function gtag(){dataLayer.push(arguments);}
				  gtag('js', new Date());
				  gtag('config', 'G-QFEP3D8PNE' );</script><link rel="canonical" href="https://learningjunho.github.io/noba-ko/multi-modal-perception.html"><link rel="alternate" type="application/atom+xml" href="https://learningjunho.github.io/noba-ko/feed.xml"><link rel="alternate" type="application/json" href="https://learningjunho.github.io/noba-ko/feed.json"><meta property="og:title" content="멀티모델 지각(Multi-Modal Perception)"><meta property="og:site_name" content="noba-ko"><meta property="og:description" content="멀티모델 지각 By Lorin Lachs California State University, Fresno 대부분의 경우, 우리는 세상을 여러 감각 양식에서 나오는 감각의 통합된 묶음으로 인식합니다. 다시 말해,&hellip;"><meta property="og:url" content="https://learningjunho.github.io/noba-ko/multi-modal-perception.html"><meta property="og:type" content="article"><link rel="stylesheet" href="https://learningjunho.github.io/noba-ko/assets/css/fontawesome-all.min.css?v=dbf9d822cefe851ba6f66e1ad57e8987"><link rel="stylesheet" href="https://learningjunho.github.io/noba-ko/assets/css/style.css?v=ab9404217d48a7e03be67391f59606ae"><noscript><link rel="stylesheet" href="https://learningjunho.github.io/noba-ko/assets/css/noscript.css?v=6228c7eee614cd200a2cad8333b439fa"></noscript><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://learningjunho.github.io/noba-ko/multi-modal-perception.html"},"headline":"멀티모델 지각(Multi-Modal Perception)","datePublished":"2023-07-02T06:38","dateModified":"2023-07-03T20:09","description":"멀티모델 지각 By Lorin Lachs California State University, Fresno 대부분의 경우, 우리는 세상을 여러 감각 양식에서 나오는 감각의 통합된 묶음으로 인식합니다. 다시 말해,&hellip;","author":{"@type":"Person","name":"learningjunho","url":"https://learningjunho.github.io/noba-ko/authors/learningjunho/"},"publisher":{"@type":"Organization","name":"learningjunho"}}</script><style>#wrapper > .bg {
               background-image: url(https://learningjunho.github.io/noba-ko/assets/images/overlay.png), linear-gradient(0deg, rgba(0, 0, 0, 0.1), rgba(0, 0, 0, 0.1)), url();
           }</style><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript></head><body class="is-preload"><div id="wrapper"><header id="header"><a class="logo" href="https://learningjunho.github.io/noba-ko/">noba-ko</a></header><nav id="nav"><ul class="links"><li><a href="https://learningjunho.github.io/noba-ko/" target="_self">홈</a></li><li><a href="https://learningjunho.github.io/noba-ko/tags/beonyeog-wanryo/" target="_self">번역 완료</a></li></ul></nav><main id="main"><article class="post"><header class="major"><time datetime="2023-07-02T06:38" class="date">7월 2, 2023</time><h1>멀티모델 지각(Multi-Modal Perception)</h1><p class="post__inner"></p></header><div class="post__inner post__entry"><article class="noba-chapter noba-chapter-enhanced"><header id="abstract"><h1>멀티모델 지각</h1>By <a href="https://nobaproject.com/authors/lorin-lachs" rel="author">Lorin Lachs</a><p class="text-muted">California State University, Fresno</p></header><section><p class="lead">대부분의 경우, 우리는 세상을 여러 감각 양식에서 나오는 감각의 통합된 묶음으로 인식합니다. 다시 말해, 우리의 지각은 다중 모달입니다. 이 모듈에서는 다중 모드 지각의 신경생물학 및 심리적 효과에 대한 정보를 포함하여 다중 모드 지각에 대한 개요를 제공합니다.</p></section><nav class="navbar navbar-default noba-navbar-action" role="toolbar" aria-label="Share &amp; Download Module"><div class="navbar-header"> </div><div id="noba-navbar-action" class="collapse navbar-collapse"><div class="collapse-wrapper"><ul class="nav navbar-nav"><li><a class="share share-facebook" data-share-receipt-service="facebook" data-share-receipt-url="https://nobaproject.com/modules/multi-modal-perception/share" href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Fnoba.to%2Fcezw4qyn" onclick="window.open(this.href, '', 'width=626,height=436');"><img loading="lazy" title="Share on Facebook" src="https://nobaproject.com/assets/social/share/facebook@2x-14b7f010fdaab7751eaff49c702d45851296a56967f4fbdc12170671bf594d7f.png" alt="Share on Facebook" width="42" height="42" data-is-external-image="true"></a></li><li><a class="share share-twitter" data-share-receipt-service="twitter" data-share-receipt-url="https://nobaproject.com/modules/multi-modal-perception/share" href="https://twitter.com/intent/tweet?url=http%3A%2F%2Fnoba.to%2Fcezw4qyn&amp;text=This%20is%20the%20future%20of%20textbooks%3A%C2%A0Multi-Modal%20Perception" onclick="window.open(this.href, '', 'width=550,height=460,scrollbars=true');"><img loading="lazy" title="Share on Twitter" src="https://nobaproject.com/assets/social/share/twitter@2x-f5c62500fccc964637cab4e89196b7c44e7ef64b8654ee123dfd48448232bd52.png" alt="Share on Twitter" width="42" height="42" data-is-external-image="true"></a></li><li><a class="share share-email" data-toggle="modal" href="https://nobaproject.com/modules/multi-modal-perception#modal-email_22"><img loading="lazy" title="Share via Email" src="https://nobaproject.com/assets/social/share/email@2x-a8982d45249ff3b3e437d1f1125dd1faf1e5649f091e9c7eabfb1a8a1a3c4521.png" alt="Share via Email" width="42" height="42" data-is-external-image="true"></a></li></ul><form class="navbar-form navbar-left share share-url"><label class="sr-only">Share this URL</label><input class="form-control" readonly="readonly" type="text" value="http://noba.to/cezw4qyn"></form></div></div></nav><section id="tags"><ul class="tags"><li><a href="https://nobaproject.com/browse-content?tags=32">Audiovisual speech</a></li><li><a href="https://nobaproject.com/browse-content?tags=33">Crossmodal</a></li><li><a href="https://nobaproject.com/browse-content?tags=30">Multimodal perception</a></li><li><a href="https://nobaproject.com/browse-content?tags=31">Multisensory</a></li></ul></section><section><h2 id="learning-objectives">학습 목표</h2><ul><li>Define the basic terminology and basic principles of multimodal perception.</li><li>Describe the neuroanatomy of multisensory integration and name some of the regions of the cortex and midbrain that have been implicated in multisensory processing.</li><li>Explain the difference between multimodal phenomena and crossmodal phenomena.</li><li>Give examples of multimodal and crossmodal behavioral effects.</li></ul><p>다중 모드 지각의 기본 용어와 기본 원리를 정의합니다.<br>다중 감각 통합의 신경 해부학적 구조를 설명하고 다중 감각 처리에 관여하는 피질과 중뇌의 일부 영역의 이름을 말하세요.<br>다중 모달 현상과 교차 모달 현상의 차이점을 설명하십시오.<br>다중 모드 및 교차 모드 행동 효과의 예를 제시하십시오.</p></section><section class="content"><h1 id="perception-unified">인식: 통합</h1><p>다양한 감각을 독립적으로 연구하는 것이 전통적이지만, 대부분의 경우 지각은 여러 감각 양식이 동시에 제공하는 정보의 맥락에서 작동합니다. 예를 들어 자동차 충돌 사고를 목격했다고 가정해 봅시다. 이 사건으로 인해 발생한 자극을 각 감각을 독립적으로, 즉 단일 모달 자극의 집합으로 간주하여 설명할 수 있습니다. 눈은 충돌한 자동차에서 반사되는 빛 에너지 패턴으로 자극을 받을 것입니다. 귀는 충돌로 인해 발생하는 음향 에너지 패턴으로 자극을 받을 수 있습니다. 코는 고무나 휘발유 타는 냄새로 자극을 받을 수도 있습니다. </p><figure data-align="left"><img loading="lazy" title="A three-car crash on the highway." src="https://nobaproject.com/images/shared/images/000/002/645/original.jpg" alt="A three-car crash on the highway." data-is-external-image="true"><figcaption>If you were a witness to this scene you'd be able to describe it using input from many of your senses. Your experience would be multimodal. [Image: Photo Grrrrr, <a href="https://goo.gl/dzfKs8,">https://goo.gl/dzfKs8,</a> CC BY-NC-SA 2.0, <a href="https://goo.gl/Toc0ZF%5D">https://goo.gl/Toc0ZF]</a></figcaption></figure><p>그러나 이 모든 정보는 모두 같은 것, 즉 자동차 충돌에 대한 지각과 관련이 있습니다. 실제로 누군가 명시적으로 단일 모달 용어로 지각을 설명해 달라고 요청하지 않는 한, 여러분은 그 사건을 여러 감각의 통합된 감각 묶음으로 경험할 가능성이 높습니다. 다시 말해, 여러분의 지각은 다중 모드일 것입니다. 문제는 이러한 다중 모드 자극과 관련된 다양한 정보 소스가 지각 시스템에 의해 개별적으로 처리되는지 여부입니다.<br><br>지난 수십 년 동안 지각 연구는 다중 모드 지각의 중요성, 즉 둘 이상의 감각 양식에서 정보가 있을 때 관찰되는 세상의 사건과 사물에 대한 지각에 미치는 영향을 지적해 왔습니다. 이 연구의 대부분은 지각 처리의 어느 시점에서 다양한 감각 양식의 정보가 통합된다는 것을 나타냅니다. 즉, 정보가 결합되어 세계를 하나의 단일 표현으로 취급한다는 것입니다.</p><h1 id="questions-about-multimodal-perception">멀티모달 인식에 관한 질문</h1><p>다중 모드 지각은 몇 가지 이론적 문제를 제기합니다. 결국, 세상은 빛, 소리, 열, 압력 등으로 우리의 지각 시스템을 끊임없이 공격하는 '시끌벅적하고 윙윙거리는 혼란의 세계'입니다. 문제를 더 복잡하게 만드는 것은 이러한 자극이 시공간을 넘나드는 여러 사건에서 비롯된다는 점입니다. 예시로 돌아가서: 여러분이 목격한 교통사고가 마을의 메인 스트리트에서 일어났다고 가정해 봅시다. 자동차 충돌이 일어나는 동안 여러분의 지각에는 자동차 충돌과 관련이 없는 많은 자극이 포함될 수 있습니다. 예를 들어, 근처 커플의 대화를 엿듣거나, 나무로 날아가는 새를 보거나, 근처 빵집에서 갓 구운 빵의 맛있는 냄새를 맡을 수도 있습니다(또는 이 세 가지를 모두 맡을 수도 있습니다!). 하지만 이러한 자극을 자동차 사고와 연관 짓는 실수는 하지 않을 가능성이 높습니다. 사실, 우리는 한 사건과 관련된 청각 자극을 다른 사건과 관련된 시각 자극과 결합하는 경우는 거의 없습니다(복화술과 같은 일부 특수한 상황에서는 결합하지만). 뇌는 어떻게 서로 다른 감각 양식에서 정보를 가져와 적절하게 일치시켜 함께 속하는 자극은 함께 유지하고 함께 속하지 않는 자극은 따로 처리할 수 있을까요? 다시 말해, 지각 시스템은 어떤 단일 모달 자극을 통합해야 하고 어떤 자극을 통합하지 않아야 하는지 어떻게 결정할까요?<br><br>단일 모드 자극이 적절하게 통합되면 이러한 통합의 결과에 대해 추가로 질문할 수 있습니다: 지각 처리가 단일 모달로만 이루어졌다면 존재하지 않았을 다중 모달 지각의 효과는 무엇일까요? 아마도 다중 모드 지각 연구에서 가장 강력한 발견은 이 마지막 질문과 관련이 있을 것입니다. 뉴런의 행동이든 개인의 행동이든 상관없이, 일반적으로 다중 모드 자극에 대한 반응은 두 가지 모달리티를 독립적으로 결합한 반응보다 더 크다는 사실이 밝혀졌습니다. 즉, 한 번에 한 가지 방식으로 자극을 제시하고 각각의 단일 모드 자극에 대한 반응을 측정한 경우, 이를 모두 더해도 다중 모드 자극에 대한 반응과 같지 않다는 것을 알 수 있습니다. 이러한 다중 감각 통합의 초가산 효과는 다중 모달 자극의 통합 처리로 인한 결과가 있음을 나타냅니다.</p><p>초첨가 효과(다중 감각 향상이라고도 함)의 정도는 가장 큰 효과를 내는 단일 자극 양식에 대한 반응의 강도에 따라 결정됩니다. 이 개념을 이해하려면 시끄러운 환경(예: 붐비는 파티)에서 누군가 나에게 말을 건네는 상황을 상상해 보세요. 이러한 유형의 다중 모드 자극에 대해 논의할 때는 단일 모드 구성 요소로 설명하는 것이 유용할 때가 많습니다: 이 경우 청각적 구성 요소(상대방의 말에서 발생하는 소리)와 시각적 구성 요소(상대방이 말을 할 때 얼굴이 움직이는 시각적 형태)가 있습니다. 사람이 많은 파티에서는 주변 소음으로 인해 청각적 요소의 말소리를 처리하기 어려울 수 있습니다. 이러한 상황에서는 연설에 대한 시각적 정보(낭독)가 연설자의 메시지를 이해하는 데 도움이 될 가능성이 매우 큽니다. 그러나 조용한 도서관에서 같은 사람이 말하는 것을 듣는다면 청각적인 부분만으로도 메시지를 받아들이기에 충분할 것이며 시각적인 부분은 거의 도움이 되지 않을 것입니다(Sumby &amp; Pollack, 1954). 일반적으로 다중 모드 구성 요소가 있는 자극의 경우 각 구성 요소 자체에 대한 반응이 약하면 다중 감각을 향상시킬 기회가 매우 큽니다. 그러나 하나의 구성 요소만으로도 강한 반응을 불러일으키기에 충분하다면 다중 감각 강화의 기회는 상대적으로 작습니다. 다중 감각 강화의 효과는 효과가 가장 큰 단일 모드 반응과 반비례하기 때문에 이 발견을 역효과의 원리(Stein &amp; Meredith, 1993)라고 부릅니다.<br><br>다중 모드 지각에 대한 또 다른 중요한 이론적 질문은 이를 뒷받침하는 신경 생물학에 관한 것입니다. 결국, 어느 시점에서 각 감각 양식의 정보는 확실히 분리됩니다(예: 빛은 눈을 통해 들어오고 소리는 귀를 통해 들어옴). 뇌는 어떻게 서로 다른 신경 시스템(시각, 청각 등)에서 정보를 가져와 결합할까요? 세상에 대한 우리의 경험이 다중 모달이라면 지각 처리 중 어느 시점에서 눈, 귀, 피부와 같은 개별 감각 기관에서 들어오는 단일 모달 정보가 결합되는 것이 틀림없습니다. 관련 질문은 이러한 통합이 뇌의 어느 부분에서 일어나는지 묻습니다. 다음 섹션에서 이러한 질문에 대해 살펴보겠습니다.</p><h1 id="biological-bases-of-multimodal-perception">다중 모드 인식의 생물학적 기반</h1><h2 id="multisensory-neurons-and-neural-convergence">다중 감각 뉴런과 신경 융합</h2><figure data-align="right"><img loading="lazy" title="Illustration of neuron activity in the brain." src="https://nobaproject.com/images/shared/images/000/002/652/original.jpg" alt="Illustration of neuron activity in the brain." data-is-external-image="true"><figcaption>In order for us to perceive the world effectively, neurons from our various senses carry information that is integrated in the brain. [Image: DARPA, https://goo.gl/kat7ws, CC0 Public Domain, https://goo.gl/m25gce]</figcaption></figure><p>중뇌와 대뇌 피질의 놀랍도록 많은 뇌 영역이 다중 모드 지각과 관련이 있습니다. 이러한 영역에는 한 가지가 아닌 여러 감각 양식의 자극에 반응하는 뉴런이 포함되어 있습니다. 예를 들어, 상측 측두 고랑이라는 영역에는 말의 시각적 및 청각적 구성 요소에 모두 반응하는 단일 뉴런이 포함되어 있습니다(Calvert, 2001; Calvert, Hansen, Iversen, &amp; Brammer, 2001). 이러한 다중 감각 융합 영역은 서로 다른 감각에서 오는 정보의 일종의 신경 교차점이기 때문에 흥미롭습니다. 즉, 시각이나 촉각과 같이 한 번에 한 가지 감각을 처리하는 데 전념하는 뉴런이 정보를 융합 영역으로 보내면 그곳에서 함께 처리됩니다.<br><br>가장 면밀히 연구된 다중 감각 융합 영역 중 하나는 시각 및 청각 자극의 단일 모드 처리에 관여하는 영역을 포함하여 뇌의 다양한 영역에서 입력을 수신하는 상대상회(Stein &amp; Meredith, 1993)입니다(Edwards, Ginsburgh, Henkel, &amp; Stein, 1979). 흥미롭게도, 상대상회는 '방향 반응'에 관여하는데, 이는 보거나 들은 자극의 위치를 향해 시선을 이동하는 행동입니다. 상대상체의 이러한 기능을 고려할 때, 상대상체에서 다감각 뉴런이 발견되는 것은 그리 놀라운 일이 아닙니다(Stein &amp; Stanford, 2008).</p><h2 id="crossmodal-receptive-fields">크로스모달 수신 필드</h2><p>다중 감각 뉴런의 해부학과 기능에 대한 자세한 내용은 뇌가 어떻게 자극을 적절하게 통합하는지에 대한 질문에 답하는 데 도움이 됩니다. 자세한 내용을 이해하려면 뉴런의 수용 영역에 대해 논의해야 합니다. 뇌 전체에서 지각자를 바로 둘러싸고 있는 공간의 매우 특정한 영역에 제시된 자극에만 반응하는 뉴런을 찾을 수 있습니다. 이 영역을 뉴런의 수용 영역이라고 합니다. 뉴런의 수용 영역에 자극이 제시되면 해당 뉴런은 발화 속도를 높이거나 낮추는 방식으로 반응합니다. 자극이 뉴런의 수용 영역 외부에 제시되면 뉴런의 발화 속도에는 아무런 영향이 없습니다. 중요한 것은 두 개의 뉴런이 세 번째 뉴런으로 정보를 보낼 때 세 번째 뉴런의 수용 영역은 두 입력 뉴런의 수용 영역의 조합이라는 점입니다. 이를 신경 수렴이라고 하는데, 여러 뉴런의 정보가 하나의 뉴런에 수렴되기 때문입니다. 다중 감각 뉴런의 경우, 수렴은 서로 다른 감각 양식에서 이루어집니다. 따라서 다중 감각 뉴런의 수용 영역은 서로 다른 감각 경로에 위치한 뉴런의 수용 영역의 조합입니다.<br><br>이제 다중 감각 뉴런을 생성하는 신경 수렴이 입력 뉴런의 수용 영역 위치를 무시하는 방식으로 설정될 수 있습니다. 그러나 놀랍게도 이러한 교차 모드 수용 필드는 서로 겹칩니다. 예를 들어, 상대상피질의 다감각 뉴런은 시각 수용 영역과 청각 수용 영역이 있는 두 개의 단일 모드 뉴런으로부터 입력을 받을 수 있습니다. 단일 모드 수용 필드는 공간에서 동일한 위치를 가리키며, 즉 두 개의 단일 모드 뉴런이 공간의 동일한 영역에서 자극에 반응한다는 것이 밝혀졌습니다. 결정적으로, 크로스 모달 수용 영역의 중첩은 크로스 모달 자극의 통합에 중요한 역할을 합니다. 서로 다른 모달리티의 정보가 겹치는 수용 필드 내에서 들어오는 경우 동일한 위치에서 들어온 것으로 간주되어 뉴런이 과증가(강화) 반응으로 반응합니다. 따라서 뇌가 다중 모드 입력을 결합하는 데 사용하는 정보 중 일부는 자극이 발생한 공간의 위치입니다.<br><br>이 패턴은 뇌의 여러 영역에 있는 많은 다중 감각 뉴런에서 공통적으로 나타납니다. 이 때문에 연구자들은 다중 감각 통합의 공간적 원리를 정의했습니다: 다중 감각 강화는 자극의 원천이 서로 공간적으로 관련되어 있을 때 관찰됩니다. 이와 관련된 현상은 교차 모달 자극의 타이밍과 관련이 있습니다. 강화 효과는 서로 다른 감각의 입력이 서로 짧은 시간 내에 도착할 때만 다중 감각 뉴런에서 관찰됩니다(예: Recanzone, 2003).</p><h2 id="multimodal-processing-in-unimodal-cortex">유니모달 피질의 멀티모달 처리</h2><p>다중 감각 뉴런은 다중 감각 융합 영역 밖에서, 한때 단일 양식(단일 양식 피질)을 처리하는 데 전념하는 것으로 여겨졌던 뇌 영역에서도 관찰되었습니다. 예를 들어, 일차 시각 피질은 오랫동안 시각 정보 처리에만 전념하는 것으로 여겨졌습니다. 일차 시각 피질은 눈에서 들어오는 정보를 처리하는 피질의 첫 번째 정거장이므로 가장자리와 같은 매우 낮은 수준의 정보를 처리합니다. 흥미롭게도 일차 시각 피질에서 일차 청각 피질(청각 경로의 소리 정보를 처리하는 곳)과 상측 측두골(위에서 언급한 다중 감각 수렴 영역)에서 정보를 수신하는 뉴런이 발견되었습니다. 이는 시각 정보 처리가 아주 초기 단계부터 청각 정보의 영향을 받는다는 것을 의미하기 때문에 주목할 만한 결과입니다.</p><figure data-align="full"><img loading="lazy" title="Motor and Sensory Regions of the Cerebral Cortex." src="https://nobaproject.com/images/shared/images/000/002/648/original.png" alt="Motor and Sensory Regions of the Cerebral Cortex." data-is-external-image="true"><figcaption>There are zones in the human brain where sensory information comes together and is integrated such as the Auditory, Visual and Motor Cortices pictured here. [Image: BruceBlaus, https://goo.gl/UqKBI3, CC BY 3.0, https://goo.gl/b58TcB]</figcaption></figure><p>이러한 다중 모드 상호 작용이 발생하는 방식에는 두 가지가 있을 수 있습니다. 첫째, 상대적으로 처리의 후반 단계에 있는 청각 정보의 처리가 피드백을 받아 단일 모드 피질에서 시각 정보의 하위 수준 처리에 영향을 미칠 수 있습니다(McDonald, Teder-Sälejärvi, Russo, &amp; Hillyard, 2003). 또는 단일 모드 피질의 영역이 서로 직접 접촉하여(Driver &amp; Noesselt, 2008; Macaluso &amp; Driver, 2005), 다중 모드 통합이 모든 감각 처리의 기본 구성 요소일 수도 있습니다.<br><br>실제로 대뇌 피질의 다중 감각 융합 영역과 일차 피질에 분포하는 수많은 다중 감각 뉴런으로 인해 일부 연구자들은 뇌에 대한 과감한 재개념화가 필요하다고 제안했습니다(Ghazanfar &amp; Schroeder, 2006). 이들은 피질을 한 종류의 감각 정보만 처리하는 고립된 영역으로 구분해서는 안 된다고 주장합니다. 오히려 이러한 영역은 특정 양식의 정보만 처리하는 것을 선호하지만 지각자에게 도움이 될 때마다 낮은 수준의 다중 감각 처리에 참여한다고 제안합니다(Vasconcelos et al., 2011).</p><h1 id="behavioral-effects-of-multimodal-perception">멀티모달 인식의 행동 효과</h1><p>신경과학자들은 뉴런 간의 매우 단순한 상호작용을 연구하는 경향이 있지만, 대뇌피질에서 많은 교차 모드 영역을 발견했다는 사실은 우리가 세상을 경험하는 방식이 근본적으로 다중 모달이라는 것을 암시하는 것 같습니다. 위에서 설명한 것처럼 지각에 대한 우리의 직관도 이와 일치하며, 사건에 대한 우리의 지각이 각 감각 양식을 독립적으로 지각하는 것으로 제한되는 것처럼 보이지 않습니다. 오히려 우리는 세상을 지각하는 감각 방식에 관계없이 하나의 통합된 세계를 지각합니다.<br><br>신경과학자들이 이러한 통합적 경험에 관여하는 신경 메커니즘의 모든 세부 사항을 밝혀내기까지는 아마도 더 많은 연구가 필요할 것입니다. 그 동안 실험 심리학자들은 다중 모드 지각과 관련된 행동 효과에 대한 조사를 통해 다중 모드 지각에 대한 이해에 기여해 왔습니다. 이러한 효과는 크게 두 가지로 분류할 수 있습니다. 첫 번째 유형인 다중 모드 현상은 여러 감각 양식의 입력이 결합하는 현상과 이러한 결합이 지각에 미치는 영향에 관한 것입니다. 두 번째 분류인 교차 모달 현상은 한 감각 양식이 다른 감각 양식의 지각에 미치는 영향에 관한 것입니다(Spence, Senkowski, &amp; Roder, 2009).</p><h1 id="multimodal-phenomena">멀티모달 현상</h1><h2 id="audiovisual-speech">시청각 연설</h2><p>다중 모달 현상은 둘 이상의 감각 양식에서 동시에(또는 거의 동시에) 정보를 생성하는 자극과 관련이 있습니다. 위에서 설명한 것처럼 음성은 이러한 종류의 자극의 대표적인 예입니다. 사람이 말을 할 때 의미 있는 정보를 전달하는 음파를 생성합니다. 만약 인식자가 화자를 보고 있다면, 그 인식자는 의미 있는 정보를 전달하는 시각적 패턴에도 접근할 수 있습니다. 물론 립리딩을 해본 사람이라면 누구나 알다시피 시각적 음성 정보가 얼마나 많은 정보를 제공하는지에는 한계가 있습니다. 그럼에도 불구하고 시각적 음성 패턴만으로도 매우 강력한 음성 인식을 할 수 있습니다. 대부분의 사람들은 청각 장애인이 정상 청력을 가진 사람보다 립리딩을 훨씬 더 잘한다고 생각합니다. 하지만 정상 청력을 가진 사람 중 일부는 입술 읽기('음성 읽기'라고도 함)에 매우 능숙하다는 사실을 알게 되면 놀랄 수도 있습니다. 실제로 정상 청력과 청각 장애 인구 모두에서 광범위한 음성 읽기 능력이 존재합니다(Andersson, Lyxell, Rönnberg, &amp; Spens, 2001). 그러나 이러한 광범위한 성능에 대한 이유는 잘 알려져 있지 않습니다(Auer &amp; Bernstein, 2007; Bernstein, 2006; Bernstein, Auer, &amp; Tucker, 2001; Mohammed et al., 2005).</p><figure data-align="right"><img loading="lazy" title="People stand and talk in groups of 3 or 4 during a party." src="https://nobaproject.com/images/shared/images/000/002/649/original.jpg" alt="People stand and talk in groups of 3 or 4 during a party." data-is-external-image="true"><figcaption>In a noisy and poorly lit environment such as a nightclub in order to have a conversation we rely on audiovisual speech to understand others. [Image: Jeremy Keith, https://goo.gl/18sLfg, CC BY 2.0, https://goo.gl/v4Y0Zv]</figcaption></figure><p>음성에 대한 시각적 정보는 음성에 대한 청각적 정보와 어떻게 상호작용할까요? 이 질문에 대한 초기 연구 중 하나는 위의 예시처럼 시끄러운 상황에서 제시된 음성 단어의 인식 정확도를 조사하는 것이었습니다(예: 사람이 많은 파티에서 대화하는 경우). 이 현상을 실험적으로 연구하기 위해 참가자들에게 관련 없는 소음('백색 잡음', 방송국 간 조율된 라디오처럼 들리는 소리)을 제시했습니다. 백색 소음 속에는 말소리가 포함되었고, 참가자들은 그 말소리를 식별하는 과제를 수행해야 했습니다. 단어의 청각적 요소만 제시되는 조건("청각적 단독" 조건)과 청각적 요소와 시각적 요소가 모두 제시되는 조건("시청각적" 조건)의 두 가지 조건이 있었습니다. 소음 수준도 다양하여 어떤 실험에서는 단어의 크기에 비해 소음이 매우 컸고, 다른 실험에서는 단어에 비해 소음이 매우 부드러웠습니다. Sumby와 Pollack(1954)은 시청각 조건에서 음성 단어 식별의 정확도가 청각만 있는 조건보다 훨씬 높다는 사실을 발견했습니다. 또한 결과의 패턴은 역효과의 원리와 일치했습니다: 시청각 프레젠테이션을 통해 얻은 이점은 청각 단독 조건의 성적이 가장 낮을 때(즉, 소음이 가장 컸을 때) 가장 높았습니다. 이러한 소음 수준에서는 시청각적 이점이 상당히 컸습니다: 참가자가 스피커를 볼 수 있도록 하는 것은 소음 볼륨을 절반 이상 낮추는 것과 같은 효과가 있는 것으로 추정되었습니다. 시청각적 이점은 분명히 행동에 극적인 영향을 미칠 수 있습니다.<br><br>시청각 연설을 사용하는 또 다른 현상은 "맥거크 효과"(발견자 중 한 사람의 이름을 따서 명명)라는 매우 유명한 착시 현상입니다. 이 착각의 고전적인 공식에서는 화자가 "가가"라는 음절을 말하는 동영상이 녹화됩니다. 또 다른 동영상은 동일한 화자가 "바바"라는 음절을 말하는 장면으로 만들어집니다. 그런 다음 "바바" 동영상의 청각 부분을 "가가" 동영상의 시각 부분에 더빙합니다. 이렇게 결합된 자극이 참가자에게 제시되고, 참가자는 영화 속 화자가 말한 내용을 보고하도록 요청받습니다. 맥거크와 맥도널드(1976)는 참가자의 98%가 자극의 시각적 또는 청각적 구성 요소에 없는 "다다"라는 음절을 들었다고 보고했습니다. 이러한 결과는 음성에 대한 시각 및 청각 정보가 통합될 때 지각에 큰 영향을 미칠 수 있음을 나타냅니다.</p><h2 id="tactilevisual-interactions-in-body-ownership">신체 소유권에서의 촉각/시각적 상호작용</h2><p>하지만 모든 다감각 통합 현상이 언어와 관련된 것은 아닙니다. 특히 매력적인 다감각 착각 중 하나는 촉각과 시각 정보가 신체 소유에 대한 인식에 통합되는 것입니다. "고무 손 착시"(Botvinick &amp; Cohen, 1998)에서 관찰자는 자신의 손 중 하나가 보이지 않도록 배치됩니다. 가려진 손 근처에 가짜 고무 손을 놓되 잘 보이는 위치에 놓습니다. 그런 다음 실험자는 가벼운 붓을 사용하여 가려진 손과 고무 손을 동시에 같은 위치에서 쓰다듬습니다. 예를 들어, 가려진 손의 가운데 손가락을 붓질하면 고무 손의 가운데 손가락도 붓질됩니다. 이렇게 하면 가려진 손의 촉각 감각과 고무 손의 시각 감각이 일치하게 됩니다. 짧은 시간(약 10분) 후, 참가자들은 고무 손이 마치 자신의 '소유'인 것처럼, 즉 고무 손이 신체의 일부인 것처럼 느껴진다고 보고합니다. 이 느낌이 너무 강해서 참가자가 고무 손을 망치로 때려서 놀라게 하면 전혀 위험하지 않은데도 반사적으로 가려진 손을 빼게 되는 경우가 종종 있습니다. 따라서 우리 몸에 대한 인식은 다감각 통합의 결과일 수 있습니다.</p><h2 id="crossmodal-phenomena">크로스모달 현상</h2><p>크로스모달 현상은 한 감각 양식이 다른 감각 양식의 지각에 미치는 영향과 관련이 있다는 점에서 멀티모달 현상과 구별됩니다.</p><h2 id="visual-influence-on-auditory-localization">청각적 로컬라이제이션에 대한 시각적 영향</h2><figure data-align="left"><img loading="lazy" title="A ventriloquist performs on stage." src="https://nobaproject.com/images/shared/images/000/002/650/original.jpg" alt="A ventriloquist performs on stage." data-is-external-image="true"><figcaption>Ventriloquists are able to trick us into believing that what we see and what we hear are the same where, in truth, they are not. [Image: Amanda Ferrell, CC0 Public Domain, https://goo.gl/m25gce]</figcaption></figure><p>유명한(그리고 흔히 경험하는) 교차 착시 현상을 "복화술 효과"라고 합니다. 복화술사가 인형에게 말을 하는 것처럼 보이면 듣는 사람은 말소리의 발신 위치가 인형의 입에 있다고 착각하게 됩니다. 즉, 우리의 지각 시스템이 청각 신호(복화술사의 입에서 나오는 소리)를 정확한 위치에 위치시키는 대신 인형의 입에 잘못 위치시키는 것입니다.<br><br>왜 이런 일이 발생할까요? 복화술사의 입에서 나오는 소리와 인형 입의 시각적 움직임이라는 자극의 두 가지 구성 요소의 위치에 대해 관찰자가 이용할 수 있는 정보를 생각해 보세요. 시각적 자극이 어디에서 오는지는 눈으로 볼 수 있기 때문에 매우 분명하지만, 소리의 위치를 정확히 파악하는 것은 훨씬 더 어렵습니다. 다시 말해, 입 움직임의 매우 정확한 시각적 위치가 청각 정보의 덜 정확한 위치보다 우선하는 것으로 보입니다. 보다 일반적으로, 다양한 청각 자극의 위치는 시각 자극의 동시 제시로 인해 영향을 받을 수 있다는 것이 밝혀졌습니다(Vroomen &amp; De Gelder, 2004). 또한 복화술 효과는 움직이는 물체에 대해서도 입증되었습니다: 시각적 물체의 움직임은 움직이는 음원의 지각된 움직임 방향에 영향을 미칠 수 있습니다(Soto-Faraco, Kingstone, &amp; Spence, 2003).</p><h2 id="auditory-influence-on-visual-perception">시각적 지각에 대한 청각의 영향</h2><p>관련 착각은 소리가 시각적 지각에 영향을 미치는 반대의 효과를 보여줍니다. 이중 플래시 착시에서는 참가자에게 컴퓨터 모니터의 중앙 지점을 응시하도록 요청합니다. 참가자의 시야의 가장자리에 흰색 원이 한 번 짧게 깜박입니다. 동시에 청각적 이벤트도 발생하는데, 신호음이 한 번 또는 두 번 연속으로 빠르게 울립니다. 놀랍게도 참가자들은 두 번의 삐 소리가 동반된 섬광을 두 번의 시각적 섬광으로 보고, 한 번의 삐 소리만 있거나 삐 소리가 없는 상황에서는 동일한 자극을 한 번의 섬광으로 본다고 보고했습니다(Shams, Kamitani, &amp; Shimojo, 2000). 다시 말해, 들리는 신호음의 수가 보이는 섬광의 수에 영향을 미친다는 것입니다!<br><br>또 다른 착각은 두 개의 원("공"이라고 함)이 서로를 향해 움직이고 서로를 통해 계속 충돌하는 것으로 인식하는 것입니다. 이러한 자극은 두 개의 공이 서로 통과하는 것으로 인식되거나 두 공이 서로 충돌한 후 반대 방향으로 튕겨져 나가는 것으로 인식될 수 있습니다. Sekuler, Sekuler, Lau(1997)는 두 공이 접촉할 때 청각 자극을 제시하는 것이 충돌 사건의 지각에 큰 영향을 미친다는 것을 보여주었습니다. 이 경우 지각된 소리가 모호한 시각 자극의 해석에 영향을 미칩니다.</p><h2 id="crossmodal-speech">크로스 모달 음성</h2><figure data-align="right"><img loading="lazy" src="https://nobaproject.com/images/shared/images/000/002/651/original.jpg" alt="" data-is-external-image="true"><figcaption>Experiments have demonstrated that by simply observing a speaker, with no auditory information, we can gather important clues about the actual sound of their voice. [Ken Whytock, https://goo.gl/VQJssP, CC BY-NC 2.0, https://goo.gl/tgFydH]</figcaption></figure><p>음성 자극에 대한 여러 가지 교차 모드 현상도 발견되었습니다. 이러한 교차 모드 음성 효과는 일반적으로 대체 단일 모드 자극(예: 시각적 패턴)에 대한 사전 경험으로 인해 단일 모드 자극(예: 청각적 패턴)에 대한 지각 처리가 변경된 것으로 나타납니다. 예를 들어, Rosenblum, Miller, Sanchez(2007)는 사람의 목소리에 익숙해지는 능력을 조사하는 실험을 실시했습니다. 이들의 첫 번째 흥미로운 발견은 단방향성입니다. 사람의 말을 반복적으로 들을 때 일어나는 것과 마찬가지로, 지각자는 화자의 '시각적 목소리'에 익숙해질 수 있습니다. 즉, 상대방이 말하는 것을 보는 것만으로도 그 사람의 말하기 스타일에 익숙해질 수 있다는 것입니다. 더욱 놀라운 것은 크로스 모달 연구 결과입니다: 이러한 시각적 정보에 익숙해지면 참가자들이 한 번도 접해본 적이 없는 화자의 청각적 연설에 대한 인식도 높아졌습니다.<br><br>마찬가지로, 지각자는 말하는 얼굴을 볼 때 그 화자의 (청각만으로) 목소리를 식별할 수 있으며, 그 반대의 경우도 마찬가지입니다(Kamachi, Hill, Lander, &amp; Vatikiotis-Bateson, 2003; Lachs &amp; Pisoni, 2004a, 2004b, 2004c; Rosenblum, Smith, Nichols, Lee, &amp; Hale, 2006). 즉, 말하기 행위를 하는 화자의 시각적 형태에는 그 화자가 어떤 소리를 내야 하는지에 대한 정보가 포함되어 있는 것으로 보입니다. 더 놀라운 사실은 청각적 형태의 말하기에도 화자가 어떤 모습이어야 하는지에 대한 정보가 포함되어 있는 것으로 보인다는 것입니다.</p><h1 id="conclusion">결론</h1><p>이 모듈에서는 우리가 세상을 경험하는 데 있어 다중 모드 지각의 역할에 관한 주요 증거와 연구 결과를 살펴봤습니다. 우리의 신경계(특히 대뇌피질)는 여러 감각에서 들어오는 정보를 처리하기 위한 상당한 구조를 가지고 있는 것으로 보입니다. 이러한 신경생물학적 구조와 다중 모드 자극과 관련된 행동 현상의 다양성을 고려할 때, 다중 모드 지각에 대한 연구는 앞으로 수년 동안 실험적 지각 분야에서 계속 관심의 대상이 될 것입니다.</p></section><section><h2 id="outside-resources">Outside Resources</h2><dl class="noba-chapter-resources"><dt>Article: A review of the neuroanatomy and methods associated with multimodal perception:</dt><dd><a href="https://dx.doi.org/10.1016/j.neubiorev.2011.04.015">http://dx.doi.org/10.1016/j.neubiorev.2011.04.015</a></dd><dt>Journal: Experimental Brain Research Special issue: Crossmodal processing</dt><dd><a href="http://www.springerlink.com/content/0014-4819/198/2-3">http://www.springerlink.com/content/0014-4819/198/2-3</a></dd><dt>TED Talk: Optical Illusions</dt><dd><a href="http://www.ted.com/talks/beau_lotto_optical_illusions_show_how_we_see">http://www.ted.com/talks/beau_lotto_optical_illusions_show_how_we_see</a></dd><dt>Video: McGurk demo</dt><dd><div class="video"><div class="post__iframe"><iframe loading="lazy" src="https://www.youtube.com/embed/aFPtc8BVdJk?color=red&amp;modestbranding=1&amp;showinfo=0&amp;origin=https://nobaproject.com&amp;theme=light" type="text/html" frameborder="0" data-mce-fragment="1"></iframe></div></div></dd><dt>Video: The Rubber Hand Illusion</dt><dd><div class="video"><div class="post__iframe"><iframe loading="lazy" src="https://www.youtube.com/embed/sxwn1w7MJvk?color=red&amp;modestbranding=1&amp;showinfo=0&amp;origin=https://nobaproject.com&amp;theme=light" type="text/html" frameborder="0" data-mce-fragment="1"></iframe></div></div></dd><dt>Web: Double-flash illusion demo</dt><dd><a href="http://www.cns.atr.jp/~kmtn/soundInducedIllusoryFlash2/">http://www.cns.atr.jp/~kmtn/soundInducedIllusoryFlash2/</a></dd></dl></section><section><h2 id="discussion-questions">Discussion Questions</h2><ol><li>The extensive network of multisensory areas and neurons in the cortex implies that much perceptual processing occurs in the context of multiple inputs. Could the processing of unimodal information ever be useful? Why or why not?</li><li>Some researchers have argued that the Principle of Inverse Effectiveness (PoIE) results from ceiling effects: Multisensory enhancement cannot take place when one modality is sufficient for processing because in such cases it is not possible for processing to be enhanced (because performance is already at the “ceiling”). On the other hand, other researchers claim that the PoIE stems from the perceptual system’s ability to assess the relative value of stimulus cues, and to use the most reliable sources of information to construct a representation of the outside world. What do you think? Could these two possibilities ever be teased apart? What kinds of experiments might one conduct to try to get at this issue?</li><li>In the late 17th century, a scientist named William Molyneux asked the famous philosopher John Locke a question relevant to modern studies of multisensory processing. The question was this: Imagine a person who has been blind since birth, and who is able, by virtue of the sense of touch, to identify three dimensional shapes such as spheres or pyramids. Now imagine that this person suddenly receives the ability to see. Would the person, without using the sense of touch, be able to identify those same shapes visually? Can modern research in multimodal perception help answer this question? Why or why not? How do the studies about crossmodal phenomena inform us about the answer to this question?</li></ol></section><section><h2 id="vocabulary">Vocabulary</h2><dl class="noba-chapter-vocabulary"><dt id="vocabulary-bouncing-balls-illusion" data-term="bouncing-balls-illusion">Bouncing balls illusion</dt><dd>The tendency to perceive two circles as bouncing off each other if the moment of their contact is accompanied by an auditory stimulus.</dd><dt id="vocabulary-crossmodal-phenomena" data-term="crossmodal-phenomena">Crossmodal phenomena</dt><dd>Effects that concern the influence of the perception of one sensory modality on the perception of another.</dd><dt id="vocabulary-crossmodal-receptive-field" data-term="crossmodal-receptive-field">Crossmodal receptive field</dt><dd>A receptive field that can be stimulated by a stimulus from more than one sensory modality.</dd><dt id="vocabulary-crossmodal-stimulus" data-term="crossmodal-stimulus">Crossmodal stimulus</dt><dd>A stimulus with components in multiple sensory modalties that interact with each other.</dd><dt id="vocabulary-double-flash-illusion" data-term="double-flash-illusion">Double flash illusion</dt><dd>The false perception of two visual flashes when a single flash is accompanied by two auditory beeps.</dd><dt id="vocabulary-integrated" data-term="integrated">Integrated</dt><dd>The process by which the perceptual system combines information arising from more than one modality.</dd><dt id="vocabulary-mcgurk-effect" data-term="mcgurk-effect">McGurk effect</dt><dd>An effect in which conflicting visual and auditory components of a speech stimulus result in an illusory percept.</dd><dt id="vocabulary-multimodal" data-term="multimodal">Multimodal</dt><dd>Of or pertaining to multiple sensory modalities.</dd><dt id="vocabulary-multimodal-perception" data-term="multimodal-perception">Multimodal perception</dt><dd>The effects that concurrent stimulation in more than one sensory modality has on the perception of events and objects in the world.</dd><dt id="vocabulary-multimodal-phenomena" data-term="multimodal-phenomena">Multimodal phenomena</dt><dd>Effects that concern the binding of inputs from multiple sensory modalities.</dd><dt id="vocabulary-multisensory-convergence-zones" data-term="multisensory-convergence-zones">Multisensory convergence zones</dt><dd>Regions in the brain that receive input from multiple unimodal areas processing different sensory modalities.</dd><dt id="vocabulary-multisensory-enhancement" data-term="multisensory-enhancement">Multisensory enhancement</dt><dd>See “superadditive effect of multisensory integration.”</dd><dt id="vocabulary-primary-auditory-cortex" data-term="primary-auditory-cortex">Primary auditory cortex</dt><dd>A region of the cortex devoted to the processing of simple auditory information.</dd><dt id="vocabulary-primary-visual-cortex" data-term="primary-visual-cortex">Primary visual cortex</dt><dd>A region of the cortex devoted to the processing of simple visual information.</dd><dt id="vocabulary-principle-of-inverse-effectiveness" data-term="principle-of-inverse-effectiveness">Principle of Inverse Effectiveness</dt><dd>The finding that, in general, for a multimodal stimulus, if the response to each unimodal component (on its own) is weak, then the opportunity for multisensory enhancement is very large. However, if one component—by itself—is sufficient to evoke a strong response, then the effect on the response gained by simultaneously processing the other components of the stimulus will be relatively small.</dd><dt id="vocabulary-receptive-field" data-term="receptive-field">Receptive field</dt><dd>The portion of the world to which a neuron will respond if an appropriate stimulus is present there.</dd><dt id="vocabulary-rubber-hand-illusion" data-term="rubber-hand-illusion">Rubber hand illusion</dt><dd>The false perception of a fake hand as belonging to a perceiver, due to multimodal sensory information.</dd><dt id="vocabulary-sensory-modalities" data-term="sensory-modalities">Sensory modalities</dt><dd>A type of sense; for example, vision or audition.</dd><dt id="vocabulary-spatial-principle-of-multisensory-integration" data-term="spatial-principle-of-multisensory-integration">Spatial principle of multisensory integration</dt><dd>The finding that the superadditive effects of multisensory integration are observed when the sources of stimulation are spatially related to one another.</dd><dt id="vocabulary-superadditive-effect-of-multisensory-integration" data-term="superadditive-effect-of-multisensory-integration">Superadditive effect of multisensory integration</dt><dd>The finding that responses to multimodal stimuli are typically greater than the sum of the independent responses to each unimodal component if it were presented on its own.</dd><dt id="vocabulary-unimodal" data-term="unimodal">Unimodal</dt><dd>Of or pertaining to a single sensory modality.</dd><dt id="vocabulary-unimodal-components" data-term="unimodal-components">Unimodal components</dt><dd>The parts of a stimulus relevant to one sensory modality at a time.</dd><dt id="vocabulary-unimodal-cortex" data-term="unimodal-cortex">Unimodal cortex</dt><dd>A region of the brain devoted to the processing of information from a single sensory modality.</dd></dl></section><section><h2 id="references">References</h2><ul class="noba-chapter-references"><li id="reference-1" data-reference="1">Andersson, U., Lyxell, B., Rönnberg, J., &amp; Spens, K.-E. (2001). Cognitive correlates of visual speech understanding in hearing-impaired individuals. Journal of Deaf Studies and Deaf Education, 6(2), 103–116. doi: 10.1093/deafed/6.2.103</li><li id="reference-2" data-reference="2">Auer, E. T., Jr., &amp; Bernstein, L. E. (2007). Enhanced visual speech perception in individuals with early-onset hearing impairment. Journal of Speech, Language, and Hearing Research, 50(5), 1157–1165. doi: 10.1044/1092-4388(2007/080)</li><li id="reference-3" data-reference="3">Bernstein, L. E. (2006). Visual speech perception. In E. Vatikiotis-Bateson, G. Bailley, &amp; P. Perrier (Eds.), Audio-visual speech processing. Cambridge, MA: MIT Press.</li><li id="reference-4" data-reference="4">Bernstein, L. E., Auer, E. T., Jr., &amp; Tucker, P. E. (2001). Enhanced speechreading in deaf adults: Can short-term training/practice close the gap for hearing adults? Journal of Speech, Language, and Hearing Research, 44, 5–18.</li><li id="reference-5" data-reference="5">Botvinick, M., &amp; Cohen, J. (1998). Rubber hands /`feel/' touch that eyes see. [10.1038/35784]. Nature, 391(6669), 756–756.</li><li id="reference-6" data-reference="6">Calvert, G. A. (2001). Crossmodal processing in the human brain: Insights from functional neuroimaging studies. Cerebral Cortex, 11, 1110–1123.</li><li id="reference-7" data-reference="7">Calvert, G. A., Hansen, P. C., Iversen, S. D., &amp; Brammer, M. J. (2001). Detection of audio-visual integration sites in humans by application of electrophysiological criteria to the bold effect. NeuroImage, 14(2), 427–438. doi: 10.1006/nimg.2001.0812</li><li id="reference-8" data-reference="8">Driver, J., &amp; Noesselt, T. (2008). Multisensory interplay reveals crossmodal influences on ‘sensory-specific’ brain regions, neural responses, and judgments. Neuron, 57(1), 11–23. doi: 10.1016/j.neuron.2007.12.013</li><li id="reference-9" data-reference="9">Edwards, S. B., Ginsburgh, C. L., Henkel, C. K., &amp; Stein, B. E. (1979). Sources of subcortical projections to the superior colliculus in the cat. Journal of Comparative Neurology, 184(2), 309–329. doi: 10.1002/cne.901840207</li><li id="reference-10" data-reference="10">Ghazanfar, A. A., &amp; Schroeder, C. E. (2006). Is neocortex essentially multisensory? TRENDS in Cognitive Sciences, 10(6), 278-285. doi: 10.1016/j.tics.2006.04.008</li><li id="reference-11" data-reference="11">Kamachi, M., Hill, H., Lander, K., &amp; Vatikiotis-Bateson, E. (2003). "Putting the face to the voice": Matching identity across modality. Current Biology, 13, 1709–1714.</li><li id="reference-12" data-reference="12">Lachs, L., &amp; Pisoni, D. B. (2004a). Crossmodal source identification in speech perception. Ecological Psychology, 16(3), 159–187.</li><li id="reference-13" data-reference="13">Lachs, L., &amp; Pisoni, D. B. (2004b). Crossmodal source information and spoken word recognition. Journal of Experimental Psychology: Human Perception &amp; Performance, 30(2), 378–396.</li><li id="reference-14" data-reference="14">Lachs, L., &amp; Pisoni, D. B. (2004c). Specification of crossmodal source information in isolated kinematic displays of speech. Journal of the Acoustical Society of America, 116(1), 507–518.</li><li id="reference-15" data-reference="15">Macaluso, E., &amp; Driver, J. (2005). Multisensory spatial interactions: A window onto functional integration in the human brain. Trends in Neurosciences, 28(5), 264–271. doi: 10.1016/j.tins.2005.03.008</li><li id="reference-16" data-reference="16">McDonald, J. J., Teder-Sälejärvi, W. A., Russo, F. D., &amp; Hillyard, S. A. (2003). Neural substrates of perceptual enhancement by cross-modal spatial attention. Journal of Cognitive Neuroscience, 15(1), 10–19. doi: 10.1162/089892903321107783</li><li id="reference-17" data-reference="17">McGurk, H., &amp; MacDonald, J. (1976). Hearing lips and seeing voices. Nature, 264, 746–748.</li><li id="reference-18" data-reference="18">Mohammed, T., Campbell, R., MacSweeney, M., Milne, E., Hansen, P., &amp; Coleman, M. (2005). Speechreading skill and visual movement sensitivity are related in deaf speechreaders. Perception, 34(2), 205–216.</li><li id="reference-19" data-reference="19">Recanzone, G. H. (2003). Auditory influences on visual temporal rate perception. Journal of Neurophysiology, 89(2), 1078–1093. doi: 10.1152/jn.00706.2002</li><li id="reference-20" data-reference="20">Rosenblum, L. D., Miller, R. M., &amp; Sanchez, K. (2007). Lip-read me now, hear me better later: Cross-modal transfer of talker-familiarity effects. Psychological Science, 18(5), 392–396.</li><li id="reference-21" data-reference="21">Rosenblum, L. D., Smith, N. M., Nichols, S. M., Lee, J., &amp; Hale, S. (2006). Hearing a face: Cross-modal speaker matching using isolated visible speech. Perception &amp; Psychophysics, 68, 84–93.</li><li id="reference-22" data-reference="22">Sekuler, R., Sekuler, A. B., &amp; Lau, R. (1997). Sound alters visual motion perception. [10.1038/385308a0]. Nature, 385(6614), 308–308.</li><li id="reference-23" data-reference="23">Shams, L., Kamitani, Y., &amp; Shimojo, S. (2000). Illusions. What you see is what you hear. Nature, 408(6814), 788. doi: 10.1038/35048669</li><li id="reference-24" data-reference="24">Soto-Faraco, S., Kingstone, A., &amp; Spence, C. (2003). Multisensory contributions to the perception of motion. Neuropsychologia, 41(13), 1847–1862. doi: 10.1016/s0028-3932(03)00185-4</li><li id="reference-25" data-reference="25">Spence, C., Senkowski, D., &amp; Roder, B. (2009). Crossmodal processing. [Editorial Introductory]. Exerimental Brain Research, 198(2-3), 107–111. doi: 10.1007/s00221-009-1973–4</li><li id="reference-26" data-reference="26">Stein, B. E., &amp; Meredith, M. A. (1993). The merging of the senses. Cambridge, MA: The MIT Press.</li><li id="reference-27" data-reference="27">Stein, B. E., &amp; Stanford, T. R. (2008). Multisensory integration: Current issues from the perspective of the single neuron. [10.1038/nrn2331]. Nature Reviews Neuroscience, 9(4), 255–266.</li><li id="reference-28" data-reference="28">Sumby, W. H., &amp; Pollack, I. (1954). Visual contribution of speech intelligibility in noise. Journal of the Acoustical Society of America, 26, 212–215.</li><li id="reference-29" data-reference="29">Vasconcelos, N., Pantoja, J., Belchior, H., Caixeta, F. V., Faber, J., Freire, M. A. M., . . . Ribeiro, S. (2011). Cross-modal responses in the primary visual cortex encode complex objects and correlate with tactile discrimination. Proceedings of the National Academy of Sciences, 108(37), 15408–15413. doi: 10.1073/pnas.1102780108</li><li id="reference-30" data-reference="30">Vroomen, J., &amp; De Gelder, B. (2004). Perceptual effects of cross-modal stimulation: Ventriloquism and the freezing phenomenon. In G. A. Calvert, C. Spence, &amp; B. E. Stein (Eds.), Handbook of multisensory processes. Cambridge, MA: MIT Press.</li></ul></section><section><h2 id="authors">Authors</h2><ul class="media-list"><li class="media"><figure class="media-object noba-author pull-right"><img loading="lazy" src="https://nobaproject.com/images/shared/author_photos/000/000/048/large.jpg" alt="" width="150" height="150" data-is-external-image="true"></figure><div class="media-body"><div class="media-heading">Lorin Lachs</div>Lorin Lachs, PhD is a Professor of Psychology at California State University, Fresno. His research concerns multisensory perception in speech and virtual reality. His interests include the history and philosophy of cognitive science, the neural basis of consciousness, video gaming and playing with his three children.</div></li></ul></section><section><h2 id="license">Creative Commons License</h2><small class="license"><a class="marks" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en_US"><img title="Creative Commons" src="https://nobaproject.com/assets/licensing/cc-7e377801d36ddb6d62c1c06dd07858f400efd7284459955e0de47bdb796c8658.png" alt="Creative Commons" data-is-external-image="true"><img loading="lazy" title="Attribution" src="https://nobaproject.com/assets/licensing/by-9be0271defac0fba0df496e1e35b7cd2aeaed8630b22b935ce2ea51380c98cba.png" alt="Attribution" data-is-external-image="true"><img title="Non-Commerical" src="https://nobaproject.com/assets/licensing/nc-1f33b73ce264f326ba55092ac717ed56b21800b76bbd849859eacf7d9319745f.png" alt="Non-Commerical" data-is-external-image="true"><img loading="lazy" title="Share-Alike" src="https://nobaproject.com/assets/licensing/sa-1725398b2ebf51d6d0165a63b36061120a047cceed2a5be57cf3f99ad65c3668.png" alt="Share-Alike" data-is-external-image="true"></a><span class="title">Multi-Modal Perception</span> by <a href="https://nobaproject.com/modules/multi-modal-perception#authors" property="cc:attributionName" rel="cc:attributionURL" xmlns:cc="https://creativecommons.org/ns#">Lorin Lachs</a> is licensed under a <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en_US" rel="license">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>. Permissions beyond the scope of this license may be available in our <a href="https://nobaproject.com/license-agreement" rel="cc:morePermissions" xmlns:cc="https://creativecommons.org/ns">Licensing Agreement</a>.</small></section><section><h2 id="apa">How to cite this Noba module using APA Style</h2>Lachs, L. (2023). Multi-modal perception. In R. Biswas-Diener &amp; E. Diener (Eds), <i>Noba textbook series: Psychology.</i> Champaign, IL: DEF publishers. Retrieved from <a href="http://noba.to/cezw4qyn">http://noba.to/cezw4qyn</a></section></article></div><footer class="post__inner post__footer"><p class="post__last-updated">This article was updated on 7월 3, 2023</p><div class="post__share-tag-container"><div class="post__tag"><h3>Tagged in:</h3><ul><li><a href="https://learningjunho.github.io/noba-ko/tags/gamgaggwa-jigagsensation-and-perception/">감각과 지각(Sensation and Perception)</a></li></ul></div><div class="post__share"><button class="post__share-button js-post__share-button icon" aria-label="Share button"><i class="fas fa-share-alt"></i></button><div class="post__share-popup js-post__share-popup"></div></div></div><div class="post__bio"><div><h3><a href="https://learningjunho.github.io/noba-ko/authors/learningjunho/" class="invert" rel="author">learningjunho</a></h3><p>심리학을 공부하기 위해 noba project를 한국어로 번역하는 도전을 시작했습니다. 번역에 DeepL과 GPT-4를 적극적으로 활용했습니다.</p></div></div></footer></article><div><strong>You should also read:</strong></div><div class="posts"><article><header><time datetime="2023-07-02T06:42" class="date">7월 2, 2023</time><h2><a href="https://learningjunho.github.io/noba-ko/sensation-and-perception.html">감각과 지각(Sensation and Perception)</a></h2></header><p>감각과 지각(Sensation and Perception) By Adam John Privitera Chemeketa Community College 감각과 지각에 관한 주제는 심리학에서 가장 오래되고 중요한 주제 중 하나입니다. 사람들은&hellip;</p><ul class="actions special"><li><a href="https://learningjunho.github.io/noba-ko/sensation-and-perception.html" class="button">Full Story</a></li></ul></article></div></main><footer id="copyright"><ul><li>© Massively</li><li>Design: <a href="https://html5up.net" target="_blank" rel="nofollow noopener">HTML5 UP</a></li><li>Powered by Publii</li><li>learningjunho 번역</li></ul></footer></div><script src="https://learningjunho.github.io/noba-ko/assets/js/jquery.min.js?v=7c14a783dfeb3d238ccd3edd840d82ee"></script><script src="https://learningjunho.github.io/noba-ko/assets/js/jquery.scrollex.min.js?v=f89065e3d988006af9791b44561d7c90"></script><script src="https://learningjunho.github.io/noba-ko/assets/js/jquery.scrolly.min.js?v=1ed5a78bde1476875a40f6b9ff44fc14"></script><script src="https://learningjunho.github.io/noba-ko/assets/js/browser.min.js?v=c07298dd19048a8a69ad97e754dfe8d0"></script><script src="https://learningjunho.github.io/noba-ko/assets/js/breakpoints.min.js?v=81a479eb099e3b187613943b085923b8"></script><script src="https://learningjunho.github.io/noba-ko/assets/js/util.min.js?v=4201a626f8c9b614a663b3a1d7d82615"></script><script src="https://learningjunho.github.io/noba-ko/assets/js/main.min.js?v=56233c354bd814758be8bff42f7e13a5"></script><script>/*<![CDATA[*/var images=document.querySelectorAll("img[loading]");for(var i=0;i<images.length;i++){if(images[i].complete){images[i].classList.add("is-loaded")}else{images[i].addEventListener("load",function(){this.classList.add("is-loaded")},false)}};/*]]>*/</script></body></html>